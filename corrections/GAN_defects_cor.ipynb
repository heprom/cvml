{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c6ff067",
   "metadata": {},
   "source": [
    "# Deep Convolutional Generative Adversarial Network (DCGAN)\n",
    "This tutorial will help you understand the basics to build and train a GAN model. GAN uses the principle of learning a distribution from the real data to create or generate new data that resembles or matches with the real data. Tensorflow version 2.1.0 is used to build our DCGAN model. <br>\n",
    "<br>\n",
    "This tutorial uses a manually created dataset from real casting defects. Although the casting defects like pores and shrinkages are 3D in nature, the created dataset is for 2D images such that the tutorial is simpler and faster to train. Certain slices from the image stack of real casting defects found in Ni-based superalloy are used to create this dataset manually. <br>\n",
    "<br>\n",
    "The dataset can be found in the folder `dataset`. For this tutorial, we will train our model for `40 epochs` and generate images every `2 epochs` to see the evolution of the generator during the training process. Since `40 epochs` is insufficient to train the model, we will load the already trained model `trained.h5` using tensorflow and generate few images from random input noise.\n",
    "\n",
    "As a side note, with much deeper networks (outside the scope of this tutorial since they need GPu hardware to train), it is *relatively easy* to acheive >90% (and even 95%) precision on this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c5e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statistics as stat\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import NoNorm\n",
    "from skimage import io\n",
    "from skimage.util import montage as montage2d\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e82fe56",
   "metadata": {},
   "source": [
    "module `matplotlib` is used for plotting graphs <br>\n",
    "module `skimage` will help us to read and write our images easily <br>\n",
    "module `imutils` will let us load only the images from a selected directory <br>\n",
    "module `scipy` is very powerful and useful to perform certain editing operations on our images <br>\n",
    "module `tensorflow` already has compiled syntaxes that can be used to build our machine learning models <br>\n",
    "\n",
    "The `tensorflow` module is used to build and compile the generator, discriminator and GAN model.<br>\n",
    "We will create a DCGAN class within which we will define functions to build generator and discriminator.<br>\n",
    "<br>\n",
    "The method `Sequential` is a keras syntax (backend tensorflow) to build the model layer by layer via addition.<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10720464",
   "metadata": {},
   "outputs": [],
   "source": [
    "## objects below are needed to compile generator and discriminator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "## required objects to build the Generator and discriminator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import ReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08446369",
   "metadata": {},
   "source": [
    "## Let's build our GAN model<br> \n",
    "**For the generator:**<br>\n",
    "The generator consists of a dense layer that takes in input vector and creates the first layer, followed by three blocks made up of `Conv2DTranspose` (2D Convolutional transpose), `BatchNormalization` and `ReLU` activation layer, and finally a convolution layer with sigmoid activation.<br> \n",
    "The size of the first Dense layer is chosen such that it can be reshaped into matrix form and be served as our first image into `Conv2DTranspose` layer.<br>\n",
    "Also note that the number of filters varies across layers and finally converges to a value equal to channels. This is because, the greyscale image has one channel (i.e., one image) while for a coloured image, the number of channels are three or requires three image layers (Red, Green, and Blue)\n",
    "\n",
    "**For the Discriminator:**<br>\n",
    "The architecture of Discriminator is almost an inverse of generator. It takes an input image whose size matches with the output of generator (and the real images ofcourse). Further, it contains three blocks of `Conv2D` and `LeakyReLU` activation layers. <br> \n",
    "Setting the padding as `same` ensures the size of the image is not reduced after convolution however, applying stride of 2 reduces the image size by a factor of 2 after each block. <br>\n",
    "Finally, the image is flattened into a neurons layer along with the filters and is converged to give one output in the last layer whose output corresponds to a value between 0 and 1. (0 for fake and 1 for real)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c28d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN:\n",
    "    @staticmethod\n",
    "    def build_generator(channels=1, inputDim=256):\n",
    "        model = Sequential()\n",
    "        \n",
    "        # No. of output units must match with size of first Trans-Conv layer\n",
    "        model.add(Dense(input_dim=inputDim, units=8*8*64)) \n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        # Reshape the series neurons into matrix form to serve as input for our first transposed convolution layer\n",
    "        model.add(Reshape((8, 8, 64)))\n",
    "        \n",
    "        # So, image size here, input = 8*8, output = 16*16 with 64 filters\n",
    "        model.add(Conv2DTranspose(64,(4,4),strides=(2,2),padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add(ReLU())\n",
    "\n",
    "        # Image size here, input = 16*16, output = 32*32 with 128 filters\n",
    "        model.add(Conv2DTranspose(128,(4,4),strides=(2,2),padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add(ReLU())\n",
    "        \n",
    "        # Image size here, input = 32*32, output = 64*64 with 256 filters\n",
    "        model.add(Conv2DTranspose(256,(4,4),strides=(2,2),padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add(ReLU())\n",
    "        \n",
    "        # Image size here, input = 64*64, output = 64*64 with 1 filter (greyscale image)\n",
    "        model.add(Conv2D(channels,(4,4),padding=\"same\"))\n",
    "        model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "        print(model.summary()) # prints the model architecture\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def build_discriminator(dim,alpha=0.2):\n",
    "        model = Sequential()\n",
    "        inputShape = (dim, dim, 1) #input shape = image size with the third dimension corresponding to channels\n",
    "\n",
    "        model.add(Conv2D(64,(4,4),padding=\"same\",strides=(2,2),input_shape=inputShape))\n",
    "        model.add(LeakyReLU(alpha=alpha))\n",
    "\n",
    "        model.add(Conv2D(128,(4,4),padding=\"same\",strides=(2,2)))\n",
    "        model.add(LeakyReLU(alpha=alpha))\n",
    "\n",
    "        model.add(Conv2D(128,(4,4),padding=\"same\",strides=(2,2)))\n",
    "        model.add(LeakyReLU(alpha=alpha))\n",
    "\n",
    "        model.add(Flatten()) # Flatten operation converts our matrix form neurons into series, a dense layer\n",
    "        model.add(Dropout(0.2)) \n",
    "\n",
    "        model.add(Dense(1))\n",
    "        model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "        print(model.summary())\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0363c76a",
   "metadata": {},
   "source": [
    "Let's now build our generator and discriminator model with the aide of class `DCGAN` defined above and functions `build_generator`, `build_discriminator`.<br>\n",
    "\n",
    "Adam optimizer is used to implement variable learning rate. `lr` is our learning rate and `decay` parameter ensures that learning rate decreases as the epochs increase.<br>\n",
    "We wind up constructing our discriminator by compiling the `binary_crossentropy` loss function with the `discOpt` optimizer parameters. The methods `compile`, `Adam` etc are already defined in the tensorflow module making it easier to code :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc1827",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20 # Number of epochs \n",
    "BATCH_SIZE = 32 # Number of images in each batch\n",
    "\n",
    "# Build generator and discriminator from the DCGAN class created above\n",
    "print(\"[INFO] building generator....\")\n",
    "gen = DCGAN.build_generator(channels=1)\n",
    "\n",
    "print(\"[INFO] building discriminator....\")\n",
    "disc = DCGAN.build_discriminator(64,alpha=0.2)\n",
    "discOpt = Adam(lr=0.0001, beta_1=0.5, decay = 0.0002/NUM_EPOCHS)\n",
    "disc.compile(loss=\"binary_crossentropy\", optimizer=discOpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2f1e33",
   "metadata": {},
   "source": [
    "**Note that discriminator is set non-trainable before compiling into GAN. This ensures that only generator is trained while we train GAN.** <br>\n",
    "The input for GAN is same as that of random input vector while output is discriminator's prediction on generated images by generator. The output or the loss that we obtain from this dicriminator's prediction is propagated back to tune and train the generator network.<br>\n",
    "*So on a whole, the good training of the generator depends on the discriminator.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c38ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] building GAN....\")\n",
    "disc.trainable = False\n",
    "## discriminator is set non-trainable before compiling it into GAN.\n",
    "\n",
    "# Set the size of input for GAN model as that of random noise vector.\n",
    "ganInput = Input(shape=(256,))\n",
    "# The output of GAN will be the discriminator's prediction of the generated images by generator.\n",
    "ganOutput = disc(gen(ganInput))\n",
    "# Create the GAN model by compiling input and output\n",
    "gan = Model(ganInput, ganOutput)\n",
    "\n",
    "#Compile the loss function and optimizer to finalise our GAN model\n",
    "ganOpt = Adam(lr=0.0002, beta_1=0.5)\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=ganOpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abe443b",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "Create a class `ImageReader` with a function `load` to load our images as a numpy array after performing resizing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f6a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageReader:\n",
    "    def load(self, file):\n",
    "        data = []\n",
    "        images = io.imread(file)\n",
    "        \n",
    "        fig = plt.figure(figsize=(10, 7))\n",
    "        fig.add_subplot(2, 2, 1)\n",
    "\n",
    "        plt.imshow(images[5,:,:], cmap='gray', vmin=0, vmax=255)\n",
    "        plt.show()\n",
    "\n",
    "        fig.add_subplot(2, 2, 2)\n",
    "        plt.imshow(images[18,:,:], cmap='gray', vmin=0, vmax=255)\n",
    "        plt.show()\n",
    "\n",
    "        fig.add_subplot(2, 2, 3)\n",
    "        plt.imshow(images[313,:,:], cmap='gray', vmin=0, vmax=255)\n",
    "        plt.show()\n",
    "\n",
    "        fig.add_subplot(2, 2, 4)\n",
    "        plt.imshow(images[495,:,:], cmap='gray', vmin=0, vmax=255)\n",
    "        plt.show()\n",
    "        \n",
    "        for i in images:\n",
    "            image = scipy.ndimage.zoom(i,(0.5,0.5),order=0,mode='nearest') # Resize the image\n",
    "\n",
    "            data.append(image)\n",
    "        return np.array(data) # output the image as a numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b4cf09",
   "metadata": {},
   "source": [
    "We will now call our class `ImageReader` as `ir`. <br>\n",
    "The images are loaded as numpy array `data` with the function `ir.load`. The array is 3D array where `axis 0` corresponds to number of images, `axis 1` and `axis 2` corresponds to size of the images. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28720af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we call our class to use its functions here\n",
    "ir = ImageReader()\n",
    "\n",
    "file = \"dataset01.tif\"\n",
    "\n",
    "# Use the function load from class ImageReader, which will load the image, resize it and output the images\n",
    "# as numpy array altogether\n",
    "data = ir.load(file)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071612e3",
   "metadata": {},
   "source": [
    "We now add a fourth dimension to the image data which defines our number of channels (==1 for greyscale and ==3 for colour) <br>\n",
    "Since we use activation functions that operate between fixed values `0 to 1`, we normalize the pixel values of our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e777b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data array is 3 Dimensional, we need to add one more dimension for channels\n",
    "data = np.expand_dims(data, axis=3)\n",
    "\n",
    "# Normalize the pixel values\n",
    "data = (data.astype(\"float\")) / 255.0\n",
    "data = shuffle(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2923a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] starting training....\")\n",
    "\n",
    "benchmarkNoise = tf.random.normal(shape=(2,256),mean=0.0,stddev=1.0)\n",
    "\n",
    "adversarial_loss = []\n",
    "discriminator_loss = []\n",
    "imlist=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f634e81b",
   "metadata": {},
   "source": [
    "Create the `benchmarknoise` which will be used to generate images for visualization after every 2 epochs<br>\n",
    "<br>\n",
    "## Now we will define the main loop that trains our models <br>\n",
    "The model is trained for N `epochs`. The image dataset is divided into mini batches of size 32 as defined by `BATCH_SIZE`ie. the parameters are updated or are trained for every batch. Hence, two for loops are used, one for `epochs` and the other for `batches per epoch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0309d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"[INFO] starting epoch {} of {}...\".format(epoch + 1, NUM_EPOCHS))\n",
    "    batchesPerEpoch = int(data.shape[0]/BATCH_SIZE)\n",
    "\n",
    "    for i in range(0, batchesPerEpoch):\n",
    "        dis_loss = []\n",
    "        adv_loss = []\n",
    "        \n",
    "        ## We need to train our discriminator to differentiate between REAL and FAKE images hence,\n",
    "        ## we generate fake images equal to batch size from generator and label it as zero (meaning fake)\n",
    "        \n",
    "        ## Both, the real and fake images are passed together to train the discriminator \n",
    "        \n",
    "        noise = tf.random.normal(shape=(BATCH_SIZE,256),mean=0.0,stddev=1.0)\n",
    "        genImages = gen.predict(noise,verbose=0)\n",
    "        imageBatch = data[i*BATCH_SIZE:(i +1)*BATCH_SIZE]\n",
    "\n",
    "        Train = np.concatenate((imageBatch, genImages))\n",
    "        label = ([1] * BATCH_SIZE) + ([0] * BATCH_SIZE)\n",
    "        (Train, label) = shuffle(Train, label)\n",
    "        label=np.array(label)\n",
    "\n",
    "        # Train on batch command trains our discriminator, updates its parameters and outputs the loss\n",
    "        discLoss = disc.train_on_batch(X, y)\n",
    "        dis_loss.append(discLoss)\n",
    "        \n",
    "        ## After updating discriminator, we generate more fake images and train the GAN model\n",
    "        ## Note that only generator is trained here as discriminator inside gan is set non-trainable\n",
    "        \n",
    "        noise = tf.random.normal(shape=(BATCH_SIZE,256),mean=0.0,stddev=1.0)\n",
    "        ganLoss = gan.train_on_batch(noise, np.array([1] * BATCH_SIZE)) \n",
    "        ## Note that the labels of fake images are set \"1\" here. This is done to achieve the \n",
    "        ## objective of generator to minimize the discriminator value function.. and \n",
    "        ## simultaneously train the generator to produce images that can be classified as \"1\"\n",
    "\n",
    "        adv_loss.append(ganLoss)\n",
    "        \n",
    "        print(\"[INFO] Step {}_{}: disc_loss = {:.6f}, adversarial_loss = {:.6f}\".format(epoch + 1, i, discLoss, ganLoss))\n",
    "\n",
    "        ## We save the images for every two epochs and finally visualize it as a montage\n",
    "        \n",
    "        if epoch % 2 == 0 and i == 0:\n",
    "            images = gen.predict(benchmarkNoise)\n",
    "            images = ((images * 255.0)).astype(\"uint8\")\n",
    "            image1 = images[0,:,:,0]\n",
    "            image1 = scipy.ndimage.zoom(image1,(2,2),order=0,mode='nearest')\n",
    "            image2 = images[1,:,:,0]\n",
    "            image2 = scipy.ndimage.zoom(image2,(2,2),order=0,mode='nearest')\n",
    "            plt.imshow(image1, cmap='gray', vmin=0, vmax=255)\n",
    "            plt.show()\n",
    "            imlist.append(image1)\n",
    "\n",
    "    adversarial_loss.append(stat.mean(adv_loss))\n",
    "    discriminator_loss.append(stat.mean(dis_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a7b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = montage2d(imlist,fill=(0,0),padding_width=10,grid_shape=(5,4))\n",
    "io.imshow(im)\n",
    "io.show()\n",
    "\n",
    "## The algorithm to save our model is commented below \n",
    "# gen.save(\"name.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea9a66",
   "metadata": {},
   "source": [
    "### Let's plot our adversarial and discriminator loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d2ea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=list(range(epochs))\n",
    "plt.plot(x,adversarial_loss, label=\"adversarial loss\")\n",
    "plt.plot(x,discriminator_loss, label=\"discriminator loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a54633",
   "metadata": {},
   "source": [
    "## Load a trained model to generate few images<br>\n",
    "The ML models are stored in the h5 format along with their parameters and architecture but not the history! <br>\n",
    "<br>\n",
    "Load the trained model with the below code and print the summary of the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed3334",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = tf.keras.models.load_model(\"trained.h5\")\n",
    "gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479456cd",
   "metadata": {},
   "source": [
    "Create a noise of shape `(9,256)` to generate 9 images from trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f008d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarkNoise = tf.random.normal(shape=(9,256),mean=0.0,stddev=1.0, seed=42)\n",
    "images = gen.predict(benchmarkNoise)\n",
    "images = ((images * 255.0)).astype(\"uint8\")\n",
    "imlist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e205333",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(images.shape[0]):\n",
    "    image1 = images[i,:,:,0]\n",
    "    image1 = scipy.ndimage.zoom(image1,(2,2),order=0,mode='nearest')\n",
    "    io.imshow(image1,norm=NoNorm(), vmin=0, vmax=255)\n",
    "    io.show()\n",
    "    imlist.append(image1)\n",
    "\n",
    "imlist = np.array(imlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd03e489",
   "metadata": {},
   "source": [
    "Create a montage of all the generated images and show them with the command montage2d.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f373bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = montage2d(imlist,fill=(0,0),padding_width=10,grid_shape=(3,3))\n",
    "io.imshow(im)\n",
    "io.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
