{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear model to classify the MNIST data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and get a sense of the data\n",
    "\n",
    "We start by loading our image data set: MNIST. Using the function `load_digits` of the `datasets` module of `sklearn` provide the dataset in a reduced form suitable for this practival session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the notebook in Google colab:\n",
    "https://colab.research.google.com/github/heprom/cvml/blob/main/corrections/mnist_linear_classifier_cor.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy and initialize the random seed to yield consistent results\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "mnist = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set need to be partitioned into train and test data. Here use the handy function `train_test_split` of `sklearn` to reserve 20% of the data to test your model.\n",
    "\n",
    "/!\\ The test data is to be left untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data is (1437, 64), type is float64\n",
      "shape of test data is (360, 64), type is float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(mnist['data'], mnist['target'], test_size=0.2)\n",
    "print('shape of train data is {}, type is {}'.format(X_train.shape, X_train.dtype))\n",
    "print('shape of test data is {}, type is {}'.format(X_test.shape, X_test.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observe the data points: they are in 64 bits floats but only integers values from 0 to 16. The data can therefore be safely casted to uint8 to reduce the memory footprint by a factor of 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "16.0\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16.]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.min())\n",
    "print(X_train.max())\n",
    "print(np.unique(X_train))\n",
    "X_train = X_train.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot an image using matplotlib. The function `imshow` can be used reshaping the data as $(8\\times8)$ array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'image 0 in the train set')"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD3CAYAAADmIkO7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAJqElEQVR4nO3da4xcZRnA8f8D5SI3V8JFUUMDBMV+cIPBSwJpPyhYL0lNlBgMhBoR8IPWxECMiakGvCQaGhKNhBhQQUHELCLxRuIiEKGJpFUrwQgtIHcsi5RrgccP510yWXe2u+125tnt/5dsuj0z57zvOTv/c2ZmCxOZiaR69hr2BCRNzzilooxTKso4paKMUyrKOKWiFmycEbEpIlYMex7zKSK2RcQx87StsyPitvnY1nyIiFMi4p5hz2MhWbBxZuayzBwf9jx2JCLOiIj7I+LZiBiLiEP73TczD8rM+3ZijKURkRGxZNdm23f7ayPiql3ZRmbemplvm685zUZEXBkRFw1yzPm0YONcCCJiGXAZcCZwJPAc8P2hTmo3iI6PpfmWmQvyC9gCvL99vxa4DrgKeAb4G3A88GXgceBB4NSedVcDd7f73gecO2XbFwCPAA8DnwESOK7dth/wHeAB4DHgB8Dr+szxG8BPe/5+LPAScHCf+/eOcyXwPeCmNs87gWP7rPdAW3db+3ofcDZwW5vrU8BmYGXPOq8Hftj28yHgImDvabb9wTbn7W3bG9vyceBi4HbgeeC4mY4rsAL495Sf35eAvwJPA9cC+/fZv+OAW9r9ngSu7bnt7cAfgK3APcDpbfln25xfavO+cdiP2Tk/xoc9gZ2e+P/H+QJwGrAE+HF7MH4F2Ac4B9jcs+6HWygBLKe7op3Y82B8FFgGHAD8ZEo064BfAYcCBwM3At/sM8cbgAunLNsGvKvP/afGuRV4d9unq4Fr+qy3tK27pGfZ2e3BeQ6wN3A+3ckm2u1jdFf1A4EjgPVMOUn1bGstcNWUZeN0J4VlbX777OC4ThfneuCodizvBs7rM/7P2s9yL2B/4OS2/EC6E+/qNocT6eJd1nMMLxr2Y3VnvxbTU5FbM/N3mfky3VX0cOBbmbkduAZYGhEjAJl5U2bem51bgN8Dp7TtnA5ckZmbMvM54GuTA0RE0D3Yv5iZWzPzGbqr4yf7zOkgurN9r6fpop6NX2bm+rZPVwOjs1xv0v2ZeXlmvgL8CHgTcGREHAmsBNZk5rOZ+ThwyQz70c+V7Ti9nJnbd3Bcp3NpZj6cmVvpTnL99m87cDRwVGa+kJmTb3R9BNiSmVe0OdwFXA98fI77UdJiivOxnu+fB55sD8rJv0MXCxGxMiLuiIitETEBfAg4rN3nKLqz8aTe7w+nu5r+JSIm2rq/bcunsw04ZMqyQ+ie9s3Goz3fPzc5/zl4bf12oqFt42i6K90jPftxGd0VdC56j82OjuuM82Pm/buA7mq8vr1L/+m2/GjgPZP70Mb8FPDGOe5HSbvl3b3KImI/urPrWcANmbk9IsbofvjQvQZ7S88qb+35/km60Jdl5kOzGG4T8M6esY+he836z53fg2nN9T8tehB4ETisXZV3dvuvLZ/Fcd1pmfko3TMWIuJk4OaI+BPdftySmR+Y47wXhMV05ZytfekCeQJ4OSJWAqf23P5zYHVEnBARBwBfnbwhM18FLgcuiYgjACLizRFxWp+xrgY+2n7HdyDwdbqnqrO9cs7WE8CrwKx+R5qZj9A95fxuRBwSEXtFxLERsbzPKo/RvSyY6fGyo+O60yLiExExecJ8ii66V4BfA8dHxJkRsU/7OikiTuiZ97z83ngY9rg4Wxifp4vwKeAMujd4Jm//DXAp8EfgX8Cf200vtj8vbMvviIj/AjcD0/7+LjM3AefRRfo43WvNz83vHr32lPVi4Pb29O69s1jtLLqg/kF3HH5B95p0Ote1P/8TEXf1mcOMx3UXnQTcGRHb2ja/kJmb25in0r1WfpjuafK36U4S0L0b/Y52TMbmaS4DM/nOnfpoZ+G/A/vN8imgNC/2uCvnbETExyJi34h4A92Z+EbD1KAZ5/TOpXvtdC/da5vzhzsd7Yl8WisV5ZVTKmrG33NGxKK8rI6OzvUf2uyasbHBvVE4yH2bmJgY2FiLWWZO+7tgr5xSUcYpFWWcUlHGKRVlnFJRxikVZZxSUcYpFWWcUlHGKRVlnFJRxikVZZxSUcYpFWWcUlHGKRVlnFJRxikVZZxSUcYpFWWcUlHGKRVlnFJRxikVZZxSUcYpFTXjBxkt1o9jGB8fH/YUdpsVK1YMewqaIz+OQVpgjFMqyjilooxTKso4paKMUyrKOKWijFMqyjilooxTKso4paKMUyrKOKWijFMqyjilooxTKso4paKMUyrKOKWijFMqyjilooxTKso4paKMUyrKOKWijFMqasmwJzBpdHR0YGMtX758YGMBrF69eqDjaXHwyikVZZxSUcYpFWWcUlHGKRVlnFJRxikVZZxSUcYpFWWcUlHGKRVlnFJRxikVZZxSUcYpFWWcUlHGKRVlnFJRxikVZZxSUcYpFWWcUlHGKRVlnFJRxikVZZxSUcYpFVXms1KWLl067CnsNuPj48OeghYgr5xSUcYpFWWcUlHGKRVlnFJRxikVZZxSUcYpFWWcUlHGKRVlnFJRxikVZZxSUcYpFWWcUlHGKRVlnFJRxikVZZxSUcYpFWWcUlHGKRVlnFJRxikVZZxSUcYpFVXm4xhGRkaGPYVFYdWqVQMba8OGDQMba8uWLQMbqwqvnFJRxikVZZxSUcYpFWWcUlHGKRVlnFJRxikVZZxSUcYpFWWcUlHGKRVlnFJRxikVZZxSUcYpFWWcUlHGKRVlnFJRxikVZZxSUcYpFWWcUlHGKRVlnFJRxikVVebjGPbE/93+7rBmzZqBjTU6Oroox4Iaj0evnFJRxikVZZxSUcYpFWWcUlHGKRVlnFJRxikVZZxSUcYpFWWcUlHGKRVlnFJRxikVZZxSUcYpFWWcUlHGKRVlnFJRxikVZZxSUcYpFWWcUlHGKRVlnFJRxikVFZnZ/8aI/jfOs5GRkUENxYYNGwY2FsDExMTAxlq1atXAxhobGxvYWIP+eIRBHsfMjOmWe+WUijJOqSjjlIoyTqko45SKMk6pKOOUijJOqSjjlIoyTqko45SKMk6pKOOUijJOqSjjlIoyTqko45SKMk6pKOOUijJOqSjjlIoyTqko45SKMk6pKOOUijJOqSjjlIpaMuwJTFqsnycCsHbt2oGNtXnz5oGNtXHjxoGNtW7duoGNVYVXTqko45SKMk6pKOOUijJOqSjjlIoyTqko45SKMk6pKOOUijJOqSjjlIoyTqko45SKMk6pKOOUijJOqSjjlIoyTqko45SKMk6pKOOUijJOqSjjlIoyTqko45SKiswc9hwkTcMrp1SUcUpFGadUlHFKRRmnVJRxSkX9D4snU2FcrKVKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt, cm\n",
    "\n",
    "index = 0\n",
    "plt.imshow(X_train[index].reshape((8, 8)), cmap=cm.gray)\n",
    "plt.axis('off')\n",
    "plt.title('image %d in the train set' % index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this particular dataset, the list of the categories is identical to their indices (from 0 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 0 is a 6\n"
     ]
    }
   ],
   "source": [
    "print('image {} is a {}'.format(index, y_train[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "\n",
    "Here we define our simple machine learning algorithm which takes the features $x$, multiply them be some weights $W$ and add a bias term $b$.\n",
    "\n",
    "$$f(x, W, b) = W.x + b = s$$\n",
    "\n",
    "for a given image in vector form with $d$ features, W has size (10, d) so that the product $W.X$ produces 10 numbers which are called the scores for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of W is now (10, 65)\n"
     ]
    }
   ],
   "source": [
    "# initialization with random weights\n",
    "W = 0.1 * np.random.randn(10, 64)\n",
    "b = 0.1 * np.random.randn(10)\n",
    "\n",
    "# apply the bias trick\n",
    "W = np.c_[b, W]\n",
    "print('shape of W is now {}'.format(W.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the data points are already in vector form, let's add 1 for the bias trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data is now (1437, 65)\n",
      "shape of test data is now (360, 65)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "print('shape of train data is now {}'.format(X_train.shape))\n",
    "\n",
    "X_test = np.c_[np.ones(X_test.shape[0]), X_test]\n",
    "print('shape of test data is now {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now compute the 10 scores for the `index` training image and use the max score to determine the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 10.47\n",
      "1:  9.16\n",
      "2: -3.46\n",
      "3: -3.20\n",
      "4: -4.26\n",
      "5: -5.17\n",
      "6: -2.96\n",
      "7:  5.29\n",
      "8:  2.92\n",
      "9: -4.51\n"
     ]
    }
   ],
   "source": [
    "scores = np.dot(W, X_train[index])\n",
    "# look at the individual score for each class\n",
    "for (label, score) in zip(labels, scores):\n",
    "    print('{}: {:5.2f}'.format(label, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: 0\n",
      "ground thruth: 6\n"
     ]
    }
   ],
   "source": [
    "print('prediction: {}'.format(labels[np.argmax(scores)]))\n",
    "print('ground thruth: {}'.format(labels[np.squeeze(y_train)[index]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "### Hinge loss\n",
    "\n",
    "We now need to define a way to tell the machine how happy we are with this prediction. The machine will then use this information to learn and come up with better predictions. The measure of our \"happiness\" is called a *loss function* and the process of learning the parameters (both $W$ and $b$) is called optimisation.\n",
    "\n",
    "One possibility to measure how good is the prediction is the so called Hinge Loss:\n",
    "\n",
    "$$L_i=\\sum_{j\\neq y_i}\\max(0, s_j - s_{y_i} + 1)$$\n",
    "\n",
    "Since it is inspired by linear support vector machines, this loss is also called Multi-class SVM Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can average arithmetically the losses $L_i$ for each instance $x_i$ to compute the general loss $L$ of the model.\n",
    "\n",
    "$$L=\\frac{1}{n}\\sum_i L_i(f(x_i, W), y_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0  10.47  14.43\n",
      " 1   9.16  13.12\n",
      " 2  -3.46   0.50\n",
      " 3  -3.20   0.76\n",
      " 4  -4.26   0.00\n",
      " 5  -5.17   0.00\n",
      "skipping 6\n",
      " 7   5.29   9.24\n",
      " 8   2.92   6.87\n",
      " 9  -4.51   0.00\n",
      "------------------\n",
      "hinge loss is 44.9\n"
     ]
    }
   ],
   "source": [
    "# step by step calculation of the loss\n",
    "Li = 0\n",
    "yi = np.squeeze(y_train)[index]  # ground truth target\n",
    "for j in range(10):\n",
    "    if j == yi:\n",
    "        print('skipping %d' % j)\n",
    "        continue\n",
    "    margin = max(0, scores[j] - scores[yi] + 1)\n",
    "    print('{:2d} {:6.2f} {:6.2f}'.format(j, scores[j], margin))\n",
    "    Li += margin\n",
    "print(18 * '-')\n",
    "print('hinge loss is {:.1f}'.format(Li))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we understand how the hinge loss works, we can use a more efficient implementation and include it in a reusable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.92905795787631\n",
      "44.92905795787631\n"
     ]
    }
   ],
   "source": [
    "# inline calculation of the loss\n",
    "yi = np.squeeze(y_train)[index]\n",
    "Li = np.sum([max(0, scores[j] - scores[yi] + 1) for j in range(10) if j != yi])\n",
    "print(Li)\n",
    "\n",
    "# create a function to evaluate the loss for the given W for image index in the training set\n",
    "def loss_i(W, index):\n",
    "    yi = np.squeeze(y_train)[index]  # ground truth target\n",
    "    scores = np.dot(W, X_train[index])\n",
    "    Li = np.sum([max(0, scores[j] - scores[yi] + 1) for j in range(10) if j != yi])\n",
    "    return Li\n",
    "\n",
    "print(loss_i(W, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally create a function to compute the average loss on a batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(W, batch_size=100):\n",
    "    L = 0.  # average loss\n",
    "    for index in range(batch_size):\n",
    "        L += loss_i(W, index)\n",
    "    L /= batch_size\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.79448178192577"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_batch(W, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax loss\n",
    "\n",
    "Another very popular loss function to use with multiclassification problems is the multinomial logistic or softmax loss (popular in deep learning). Here the score for each class is passed to the softmax function: exponentiated (and become positive) and normalized. This gives the probability distribution of this class:\n",
    "\n",
    "$$P(Y=k|X=x_i)=\\frac{e^{s_k}}{\\sum_j e^{s_j}}$$\n",
    "\n",
    "Now we have a probability we can try to maximize the likelihood which is equivalent to minimize the negative of the log likelihood:\n",
    "\n",
    "$$L_i=-\\log P(Y=k|X=x_i)=-\\log\\left(\\frac{e^{s_k}}{\\sum_j e^{s_j}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0 |  35410.2 | 0.7843\n",
      "     1 |   9524.5 | 0.2109\n",
      "     2 |      0.0 | 0.0000\n",
      "     3 |      0.0 | 0.0000\n",
      "     4 |      0.0 | 0.0000\n",
      "     5 |      0.0 | 0.0000\n",
      "     6 |      0.1 | 0.0000\n",
      "     7 |    197.5 | 0.0044\n",
      "     8 |     18.5 | 0.0004\n",
      "     9 |      0.0 | 0.0000\n",
      "--------------------------\n",
      "sum of probabilities check: 1.000\n",
      "Softmax loss is 13.68\n"
     ]
    }
   ],
   "source": [
    "# start by exponentiating our scores to obtain unnormalized probabilities\n",
    "escores = np.exp(scores)\n",
    "norm_escores = escores / np.sum(escores)\n",
    "for j in range(10):\n",
    "    print('{:6d} | {:8.1f} | {:6.4f}'.format(j, escores[j], norm_escores[j]))\n",
    "print(26 * '-')\n",
    "# verify that the sum of the probability is 1\n",
    "print('sum of probabilities check: {:.3f}'.format(np.sum(norm_escores)))\n",
    "# compute the softmax loss\n",
    "Li = -np.log(norm_escores[yi])\n",
    "print('Softmax loss is {:.2f}'.format(Li))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the model\n",
    "\n",
    "Here we use the calculated loss to optimize the parameters $W$ and $b$. For this we need to evaluate the gradient $\\dfrac{\\partial L}{\\partial W}$ of $L$ with respect to $W$.\n",
    "\n",
    "The gradient is obtained by differentiating the loss expression with respect to $W$:\n",
    "\n",
    "$$\\nabla_{w_j}L_i=1\\left(w_j^T x_i - w_{y_i}^T x_i + 1 > 0\\right) x_i\\quad\\text{for }j\\neq y_i$$\n",
    "\n",
    "$$\\nabla_{w_{y_i}}L_i=-\\left(\\sum_{j\\neq y_i}1\\left(w_j^T x_i - w_{y_i}^T x_i + 1 > 0\\right)\\right) x_i$$\n",
    "\n",
    "with $1(condition)$ equals to 1 if $condition$ is true, 0 otherwise. Here we see that the data vector $x$ is scaled by the number of classes that did not meet the margins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data is (1437, 65)\n",
      "shape of W is (10, 65)\n"
     ]
    }
   ],
   "source": [
    "# verify one more time the size of our matrices\n",
    "print('shape of train data is {}'.format(X_train.shape))\n",
    "print('shape of W is {}'.format(W.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Simple SVM loss gradient implementation:\n",
    " - iterate over each data point $i$ in the batch\n",
    " - compute the score using $W.x_i$ (bias trick)\n",
    " - compute the margin for each class\n",
    " - compute the loss and the gradient components associated with this data point\n",
    " - finally average the gradient and the loss with respect to the number of data points in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss_gradient(W, X, y):\n",
    "    \"\"\"\n",
    "    SVM loss gradient.\n",
    "\n",
    "    Inputs:\n",
    "    - W: array of shape (K, 1 + D) containing the weights.\n",
    "    - X: array of shape (N, 1 + D) containing the data.\n",
    "    - y: array of shape (N, 1) containing training labels 0 <= k < K.\n",
    "    Returns a tuple of:\n",
    "    - average loss\n",
    "    - gradient of the loss with respect to weights W\n",
    "    \"\"\"\n",
    "    dW = np.zeros_like(W)  # initialize the gradient as zero\n",
    "    K = W.shape[0]  # number of classes\n",
    "    n = X.shape[0]  # number of data points\n",
    "    loss = 0.0\n",
    "    for i in range(n):\n",
    "        #print('evaluating gradient / image %d' % i)\n",
    "        yi = np.squeeze(y)[i]  # ground truth target\n",
    "        scores = np.dot(W, X[i])\n",
    "        # compute SVM loss and gradient for this data point\n",
    "        for j in range(K):\n",
    "            if j == yi:\n",
    "                continue\n",
    "            # only compute loss if incorrectly classified\n",
    "            margin = max(0, scores[j] - scores[yi] + 1)\n",
    "            if margin > 0:\n",
    "                loss += margin\n",
    "                dW[yi, :] -= X[i]  # correct class gradient\n",
    "                dW[j, :] += X[i]  # incorrect class gradient\n",
    "\n",
    "    # average the loss and gradient\n",
    "    loss /= n\n",
    "    dW /= n\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now try our SVM gradient loss by computing the gradient with respect to the first `nb` images in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 41.82\n",
      "gradient dW with respect to the first pixel = [ 0.32  0.25 -0.29 -0.09  0.21 -1.13  0.14  0.29  0.13  0.17]\n"
     ]
    }
   ],
   "source": [
    "nb = 100\n",
    "loss, dW = svm_loss_gradient(W, X_train[:nb], y_train[:nb])\n",
    "print('loss is {:.2f}'.format(loss))\n",
    "print('gradient dW with respect to the first pixel =', dW[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient check\n",
    "\n",
    "now, to verify our SVM gradietn implementation, we are going to perform a **gradient check**. \n",
    "\n",
    "The gradient is computed numerically using a finite difference scheme:\n",
    "\n",
    "$$\\nabla L\\approx\\dfrac{L(W+h) - L(W-h)}{2h}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(f, W, h=0.0001):\n",
    "    dL = np.zeros_like(W)\n",
    "    # evaluate the loss modifiying each value of W\n",
    "    for c in range(W.shape[0]):\n",
    "        for p in range(W.shape[1]):\n",
    "            W[c, p] += h\n",
    "            fxph = f(W)\n",
    "            W[c, p] -= 2*h\n",
    "            fxmh = f(W)\n",
    "            dL[c, p] = (fxph - fxmh) / (2 * h)  # centered finite differences\n",
    "            W[c, p] += h  # put back initial value\n",
    "    return dL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply our gradient check, print the gradient with respect to the first pixel. Compare with the analytical value. Realize that to evaluate the gradient numerically, the loss function was called $2\\times64$ times. This is why it is so slow. And we tested it only with 100 training images over 1437!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 41.82\n",
      "(10, 65)\n"
     ]
    }
   ],
   "source": [
    "print('loss is {:.2f}'.format(loss_batch(W, batch_size=100)))\n",
    "dL = gradient_check(loss_batch, W)\n",
    "print(dL.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.32  0.25 -0.29 -0.09  0.21 -1.13  0.14  0.29  0.13  0.17]\n"
     ]
    }
   ],
   "source": [
    "print(dL[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "now we have successfully created our linear model, loss function, and that we can compute the gradient of the loss with respect to $W$, let's actually use this to perform gradient descent and learn our model.\n",
    "\n",
    "The backbone of the gradient descent is this simple equation:\n",
    "$$W\\leftarrow W - \\eta \\nabla_W L$$\n",
    "\n",
    "$\\eta$ is the learning rate (the most important hyperparameter). The weights $W$ are being updated at each iteration until a stop criterion is met or a maximum number of iteration reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss is 36.8\n",
      "after one step the average loss is 16.3\n"
     ]
    }
   ],
   "source": [
    "# examine one single gradient descent step\n",
    "W = 0.1 * np.random.randn(10, 65)\n",
    "print('average loss is %.1f' % loss_batch(W, batch_size=X_train.shape[0]))\n",
    "loss, dL_dw = svm_loss_gradient(W, X_train, y_train)\n",
    "\n",
    "# perform one gradient descent\n",
    "eta = 0.005\n",
    "W = W - eta * dL_dw\n",
    "print('after one step the average loss is %.1f' % loss_batch(W, batch_size=X_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch gradient descent\n",
    "\n",
    "because $n$ is large (1437 here), it does not actually make sense of computing the gradient on the complete set of training images at each iteration (remeber that the gradient is averaged). Instead, it is very common to compute the gradient on a subset (called a mini-batch) of 32 to 256 images. This is much faster and performs well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0 - loss 408.5\n",
      "it 1 - loss 345.4\n",
      "it 2 - loss 317.7\n",
      "it 3 - loss 301.3\n",
      "it 4 - loss 269.3\n",
      "it 5 - loss 223.9\n",
      "it 6 - loss 188.3\n",
      "it 7 - loss 171.6\n",
      "it 8 - loss 147.4\n",
      "it 9 - loss 123.2\n",
      "it 10 - loss 137.4\n",
      "it 11 - loss 121.2\n",
      "it 12 - loss 121.5\n",
      "it 13 - loss 103.8\n",
      "it 14 - loss 106.4\n",
      "it 15 - loss 106.5\n",
      "it 16 - loss 102.5\n",
      "it 17 - loss 94.5\n",
      "it 18 - loss 87.8\n",
      "it 19 - loss 78.4\n",
      "it 20 - loss 82.3\n",
      "it 21 - loss 75.4\n",
      "it 22 - loss 67.5\n",
      "it 23 - loss 76.6\n",
      "it 24 - loss 75.7\n",
      "it 25 - loss 61.1\n",
      "it 26 - loss 81.7\n",
      "it 27 - loss 66.3\n",
      "it 28 - loss 71.0\n",
      "it 29 - loss 57.6\n",
      "it 30 - loss 57.5\n",
      "it 31 - loss 54.4\n",
      "it 32 - loss 56.4\n",
      "it 33 - loss 51.6\n",
      "it 34 - loss 55.5\n",
      "it 35 - loss 51.2\n",
      "it 36 - loss 45.8\n",
      "it 37 - loss 50.1\n",
      "it 38 - loss 40.8\n",
      "it 39 - loss 38.2\n",
      "it 40 - loss 46.1\n",
      "it 41 - loss 38.6\n",
      "it 42 - loss 37.5\n",
      "it 43 - loss 43.2\n",
      "it 44 - loss 47.1\n",
      "it 45 - loss 31.0\n",
      "it 46 - loss 46.4\n",
      "it 47 - loss 44.5\n",
      "it 48 - loss 32.5\n",
      "it 49 - loss 39.1\n",
      "it 50 - loss 42.7\n",
      "it 51 - loss 39.7\n",
      "it 52 - loss 40.9\n",
      "it 53 - loss 27.3\n",
      "it 54 - loss 31.5\n",
      "it 55 - loss 36.6\n",
      "it 56 - loss 42.4\n",
      "it 57 - loss 27.3\n",
      "it 58 - loss 36.9\n",
      "it 59 - loss 25.2\n",
      "it 60 - loss 20.7\n",
      "it 61 - loss 29.7\n",
      "it 62 - loss 27.8\n",
      "it 63 - loss 26.7\n",
      "it 64 - loss 28.9\n",
      "it 65 - loss 28.1\n",
      "it 66 - loss 21.2\n",
      "it 67 - loss 35.5\n",
      "it 68 - loss 34.9\n",
      "it 69 - loss 19.6\n",
      "it 70 - loss 21.5\n",
      "it 71 - loss 29.5\n",
      "it 72 - loss 22.9\n",
      "it 73 - loss 22.4\n",
      "it 74 - loss 23.2\n",
      "it 75 - loss 23.1\n",
      "it 76 - loss 27.2\n",
      "it 77 - loss 23.2\n",
      "it 78 - loss 30.4\n",
      "it 79 - loss 20.5\n",
      "it 80 - loss 23.3\n",
      "it 81 - loss 23.5\n",
      "it 82 - loss 26.4\n",
      "it 83 - loss 20.6\n",
      "it 84 - loss 26.5\n",
      "it 85 - loss 22.1\n",
      "it 86 - loss 16.7\n",
      "it 87 - loss 15.0\n",
      "it 88 - loss 18.1\n",
      "it 89 - loss 17.5\n",
      "it 90 - loss 24.6\n",
      "it 91 - loss 21.8\n",
      "it 92 - loss 13.6\n",
      "it 93 - loss 16.1\n",
      "it 94 - loss 16.1\n",
      "it 95 - loss 21.2\n",
      "it 96 - loss 19.6\n",
      "it 97 - loss 20.0\n",
      "it 98 - loss 16.4\n",
      "it 99 - loss 16.3\n",
      "it 100 - loss 21.5\n",
      "it 101 - loss 13.2\n",
      "it 102 - loss 15.2\n",
      "it 103 - loss 14.4\n",
      "it 104 - loss 15.6\n",
      "it 105 - loss 12.8\n",
      "it 106 - loss 15.1\n",
      "it 107 - loss 14.8\n",
      "it 108 - loss 15.4\n",
      "it 109 - loss 17.4\n",
      "it 110 - loss 18.2\n",
      "it 111 - loss 14.7\n",
      "it 112 - loss 14.8\n",
      "it 113 - loss 13.6\n",
      "it 114 - loss 16.1\n",
      "it 115 - loss 11.8\n",
      "it 116 - loss 13.3\n",
      "it 117 - loss 13.8\n",
      "it 118 - loss 16.1\n",
      "it 119 - loss 20.8\n",
      "it 120 - loss 10.4\n",
      "it 121 - loss 15.8\n",
      "it 122 - loss 12.6\n",
      "it 123 - loss 13.8\n",
      "it 124 - loss 15.4\n",
      "it 125 - loss 9.3\n",
      "it 126 - loss 9.1\n",
      "it 127 - loss 10.2\n",
      "it 128 - loss 14.6\n",
      "it 129 - loss 17.8\n",
      "it 130 - loss 12.6\n",
      "it 131 - loss 11.1\n",
      "it 132 - loss 8.5\n",
      "it 133 - loss 13.5\n",
      "it 134 - loss 11.2\n",
      "it 135 - loss 13.2\n",
      "it 136 - loss 15.0\n",
      "it 137 - loss 9.9\n",
      "it 138 - loss 9.4\n",
      "it 139 - loss 20.2\n",
      "it 140 - loss 14.7\n",
      "it 141 - loss 10.9\n",
      "it 142 - loss 8.8\n",
      "it 143 - loss 10.1\n",
      "it 144 - loss 12.1\n",
      "it 145 - loss 10.4\n",
      "it 146 - loss 10.9\n",
      "it 147 - loss 9.9\n",
      "it 148 - loss 7.7\n",
      "it 149 - loss 12.8\n",
      "it 150 - loss 9.5\n",
      "it 151 - loss 11.9\n",
      "it 152 - loss 5.1\n",
      "it 153 - loss 11.0\n",
      "it 154 - loss 8.3\n",
      "it 155 - loss 10.1\n",
      "it 156 - loss 8.2\n",
      "it 157 - loss 8.4\n",
      "it 158 - loss 5.2\n",
      "it 159 - loss 9.7\n",
      "it 160 - loss 11.2\n",
      "it 161 - loss 7.7\n",
      "it 162 - loss 11.1\n",
      "it 163 - loss 11.7\n",
      "it 164 - loss 7.6\n",
      "it 165 - loss 13.5\n",
      "it 166 - loss 13.9\n",
      "it 167 - loss 14.8\n",
      "it 168 - loss 13.6\n",
      "it 169 - loss 10.0\n",
      "it 170 - loss 12.2\n",
      "it 171 - loss 12.9\n",
      "it 172 - loss 7.4\n",
      "it 173 - loss 9.4\n",
      "it 174 - loss 9.1\n",
      "it 175 - loss 9.8\n",
      "it 176 - loss 8.9\n",
      "it 177 - loss 13.2\n",
      "it 178 - loss 9.1\n",
      "it 179 - loss 13.9\n",
      "it 180 - loss 8.2\n",
      "it 181 - loss 9.9\n",
      "it 182 - loss 12.2\n",
      "it 183 - loss 4.4\n",
      "it 184 - loss 7.9\n",
      "it 185 - loss 8.3\n",
      "it 186 - loss 10.6\n",
      "it 187 - loss 7.7\n",
      "it 188 - loss 7.8\n",
      "it 189 - loss 15.6\n",
      "it 190 - loss 10.0\n",
      "it 191 - loss 6.8\n",
      "it 192 - loss 15.1\n",
      "it 193 - loss 7.0\n",
      "it 194 - loss 5.6\n",
      "it 195 - loss 4.7\n",
      "it 196 - loss 8.9\n",
      "it 197 - loss 8.6\n",
      "it 198 - loss 8.3\n",
      "it 199 - loss 11.9\n",
      "it 200 - loss 7.5\n",
      "it 201 - loss 7.9\n",
      "it 202 - loss 7.6\n",
      "it 203 - loss 6.6\n",
      "it 204 - loss 10.8\n",
      "it 205 - loss 8.7\n",
      "it 206 - loss 9.1\n",
      "it 207 - loss 9.2\n",
      "it 208 - loss 7.4\n",
      "it 209 - loss 5.2\n",
      "it 210 - loss 6.8\n",
      "it 211 - loss 8.0\n",
      "it 212 - loss 5.7\n",
      "it 213 - loss 8.8\n",
      "it 214 - loss 8.1\n",
      "it 215 - loss 10.2\n",
      "it 216 - loss 6.5\n",
      "it 217 - loss 6.3\n",
      "it 218 - loss 6.3\n",
      "it 219 - loss 10.0\n",
      "it 220 - loss 6.5\n",
      "it 221 - loss 6.7\n",
      "it 222 - loss 14.2\n",
      "it 223 - loss 8.6\n",
      "it 224 - loss 7.6\n",
      "it 225 - loss 9.0\n",
      "it 226 - loss 8.6\n",
      "it 227 - loss 7.2\n",
      "it 228 - loss 8.2\n",
      "it 229 - loss 3.8\n",
      "it 230 - loss 5.0\n",
      "it 231 - loss 7.3\n",
      "it 232 - loss 8.7\n",
      "it 233 - loss 5.9\n",
      "it 234 - loss 9.4\n",
      "it 235 - loss 5.9\n",
      "it 236 - loss 13.6\n",
      "it 237 - loss 9.0\n",
      "it 238 - loss 4.8\n",
      "it 239 - loss 6.8\n",
      "it 240 - loss 5.7\n",
      "it 241 - loss 4.4\n",
      "it 242 - loss 6.2\n",
      "it 243 - loss 5.3\n",
      "it 244 - loss 5.8\n",
      "it 245 - loss 5.1\n",
      "it 246 - loss 8.6\n",
      "it 247 - loss 6.2\n",
      "it 248 - loss 6.8\n",
      "it 249 - loss 9.1\n",
      "it 250 - loss 7.1\n",
      "it 251 - loss 7.1\n",
      "it 252 - loss 6.4\n",
      "it 253 - loss 7.8\n",
      "it 254 - loss 5.5\n",
      "it 255 - loss 10.7\n",
      "it 256 - loss 10.2\n",
      "it 257 - loss 5.9\n",
      "it 258 - loss 4.6\n",
      "it 259 - loss 5.9\n",
      "it 260 - loss 8.4\n",
      "it 261 - loss 5.2\n",
      "it 262 - loss 5.1\n",
      "it 263 - loss 3.3\n",
      "it 264 - loss 6.1\n",
      "it 265 - loss 7.1\n",
      "it 266 - loss 8.0\n",
      "it 267 - loss 5.5\n",
      "it 268 - loss 6.2\n",
      "it 269 - loss 5.3\n",
      "it 270 - loss 5.4\n",
      "it 271 - loss 5.1\n",
      "it 272 - loss 3.8\n",
      "it 273 - loss 4.7\n",
      "it 274 - loss 4.7\n",
      "it 275 - loss 5.1\n",
      "it 276 - loss 5.7\n",
      "it 277 - loss 7.8\n",
      "it 278 - loss 7.1\n",
      "it 279 - loss 4.6\n",
      "it 280 - loss 9.1\n",
      "it 281 - loss 5.7\n",
      "it 282 - loss 8.7\n",
      "it 283 - loss 4.7\n",
      "it 284 - loss 8.8\n",
      "it 285 - loss 3.9\n",
      "it 286 - loss 6.3\n",
      "it 287 - loss 5.3\n",
      "it 288 - loss 4.7\n",
      "it 289 - loss 7.1\n",
      "it 290 - loss 3.9\n",
      "it 291 - loss 5.0\n",
      "it 292 - loss 5.0\n",
      "it 293 - loss 7.6\n",
      "it 294 - loss 5.8\n",
      "it 295 - loss 3.4\n",
      "it 296 - loss 7.9\n",
      "it 297 - loss 4.6\n",
      "it 298 - loss 6.5\n",
      "it 299 - loss 5.4\n",
      "it 300 - loss 6.8\n",
      "it 301 - loss 3.9\n",
      "it 302 - loss 6.7\n",
      "it 303 - loss 6.7\n",
      "it 304 - loss 7.5\n",
      "it 305 - loss 8.4\n",
      "it 306 - loss 3.5\n",
      "it 307 - loss 4.9\n",
      "it 308 - loss 5.7\n",
      "it 309 - loss 4.6\n",
      "it 310 - loss 4.0\n",
      "it 311 - loss 6.5\n",
      "it 312 - loss 3.7\n",
      "it 313 - loss 7.7\n",
      "it 314 - loss 8.2\n",
      "it 315 - loss 4.5\n",
      "it 316 - loss 7.1\n",
      "it 317 - loss 2.8\n",
      "it 318 - loss 3.7\n",
      "it 319 - loss 5.1\n",
      "it 320 - loss 3.8\n",
      "it 321 - loss 4.2\n",
      "it 322 - loss 4.2\n",
      "it 323 - loss 7.7\n",
      "it 324 - loss 10.3\n",
      "it 325 - loss 5.7\n",
      "it 326 - loss 3.3\n",
      "it 327 - loss 7.9\n",
      "it 328 - loss 6.8\n",
      "it 329 - loss 4.5\n",
      "it 330 - loss 7.0\n",
      "it 331 - loss 4.5\n",
      "it 332 - loss 8.1\n",
      "it 333 - loss 4.9\n",
      "it 334 - loss 3.8\n",
      "it 335 - loss 7.3\n",
      "it 336 - loss 4.5\n",
      "it 337 - loss 4.3\n",
      "it 338 - loss 2.8\n",
      "it 339 - loss 6.0\n",
      "it 340 - loss 5.1\n",
      "it 341 - loss 6.1\n",
      "it 342 - loss 5.8\n",
      "it 343 - loss 5.8\n",
      "it 344 - loss 3.4\n",
      "it 345 - loss 3.6\n",
      "it 346 - loss 5.9\n",
      "it 347 - loss 3.6\n",
      "it 348 - loss 5.1\n",
      "it 349 - loss 4.4\n",
      "it 350 - loss 4.3\n",
      "it 351 - loss 4.3\n",
      "it 352 - loss 4.5\n",
      "it 353 - loss 4.2\n",
      "it 354 - loss 5.6\n",
      "it 355 - loss 5.3\n",
      "it 356 - loss 4.7\n",
      "it 357 - loss 6.1\n",
      "it 358 - loss 3.7\n",
      "it 359 - loss 5.3\n",
      "it 360 - loss 4.6\n",
      "it 361 - loss 4.8\n",
      "it 362 - loss 7.8\n",
      "it 363 - loss 3.6\n",
      "it 364 - loss 4.3\n",
      "it 365 - loss 5.4\n",
      "it 366 - loss 5.1\n",
      "it 367 - loss 6.7\n",
      "it 368 - loss 3.7\n",
      "it 369 - loss 8.9\n",
      "it 370 - loss 7.4\n",
      "it 371 - loss 6.3\n",
      "it 372 - loss 5.7\n",
      "it 373 - loss 4.9\n",
      "it 374 - loss 3.7\n",
      "it 375 - loss 6.8\n",
      "it 376 - loss 3.6\n",
      "it 377 - loss 4.5\n",
      "it 378 - loss 5.3\n",
      "it 379 - loss 4.3\n",
      "it 380 - loss 1.6\n",
      "it 381 - loss 6.1\n",
      "it 382 - loss 4.1\n",
      "it 383 - loss 5.2\n",
      "it 384 - loss 3.5\n",
      "it 385 - loss 2.9\n",
      "it 386 - loss 5.0\n",
      "it 387 - loss 2.0\n",
      "it 388 - loss 5.3\n",
      "it 389 - loss 3.9\n",
      "it 390 - loss 4.7\n",
      "it 391 - loss 2.8\n",
      "it 392 - loss 3.3\n",
      "it 393 - loss 6.1\n",
      "it 394 - loss 5.5\n",
      "it 395 - loss 5.5\n",
      "it 396 - loss 3.7\n",
      "it 397 - loss 4.6\n",
      "it 398 - loss 3.0\n",
      "it 399 - loss 3.4\n",
      "it 400 - loss 3.1\n",
      "it 401 - loss 3.2\n",
      "it 402 - loss 6.0\n",
      "it 403 - loss 4.5\n",
      "it 404 - loss 6.8\n",
      "it 405 - loss 3.9\n",
      "it 406 - loss 3.8\n",
      "it 407 - loss 3.0\n",
      "it 408 - loss 2.9\n",
      "it 409 - loss 4.9\n",
      "it 410 - loss 4.5\n",
      "it 411 - loss 5.1\n",
      "it 412 - loss 6.5\n",
      "it 413 - loss 4.3\n",
      "it 414 - loss 5.4\n",
      "it 415 - loss 4.8\n",
      "it 416 - loss 4.7\n",
      "it 417 - loss 6.5\n",
      "it 418 - loss 5.2\n",
      "it 419 - loss 4.5\n",
      "it 420 - loss 4.4\n",
      "it 421 - loss 3.3\n",
      "it 422 - loss 6.0\n",
      "it 423 - loss 2.3\n",
      "it 424 - loss 6.6\n",
      "it 425 - loss 5.2\n",
      "it 426 - loss 3.8\n",
      "it 427 - loss 4.4\n",
      "it 428 - loss 4.1\n",
      "it 429 - loss 3.6\n",
      "it 430 - loss 2.8\n",
      "it 431 - loss 4.4\n",
      "it 432 - loss 5.0\n",
      "it 433 - loss 6.5\n",
      "it 434 - loss 5.5\n",
      "it 435 - loss 4.3\n",
      "it 436 - loss 2.8\n",
      "it 437 - loss 3.1\n",
      "it 438 - loss 6.6\n",
      "it 439 - loss 4.4\n",
      "it 440 - loss 2.4\n",
      "it 441 - loss 2.9\n",
      "it 442 - loss 5.1\n",
      "it 443 - loss 5.5\n",
      "it 444 - loss 4.5\n",
      "it 445 - loss 5.3\n",
      "it 446 - loss 4.2\n",
      "it 447 - loss 3.7\n",
      "it 448 - loss 4.2\n",
      "it 449 - loss 2.5\n",
      "it 450 - loss 4.9\n",
      "it 451 - loss 3.6\n",
      "it 452 - loss 3.7\n",
      "it 453 - loss 3.2\n",
      "it 454 - loss 5.6\n",
      "it 455 - loss 4.2\n",
      "it 456 - loss 3.1\n",
      "it 457 - loss 6.0\n",
      "it 458 - loss 3.8\n",
      "it 459 - loss 3.2\n",
      "it 460 - loss 3.7\n",
      "it 461 - loss 2.5\n",
      "it 462 - loss 3.0\n",
      "it 463 - loss 2.8\n",
      "it 464 - loss 3.5\n",
      "it 465 - loss 3.8\n",
      "it 466 - loss 5.1\n",
      "it 467 - loss 4.4\n",
      "it 468 - loss 2.7\n",
      "it 469 - loss 5.8\n",
      "it 470 - loss 6.5\n",
      "it 471 - loss 2.9\n",
      "it 472 - loss 3.5\n",
      "it 473 - loss 3.6\n",
      "it 474 - loss 2.4\n",
      "it 475 - loss 3.8\n",
      "it 476 - loss 5.9\n",
      "it 477 - loss 3.4\n",
      "it 478 - loss 2.5\n",
      "it 479 - loss 3.6\n",
      "it 480 - loss 3.9\n",
      "it 481 - loss 3.9\n",
      "it 482 - loss 3.7\n",
      "it 483 - loss 4.2\n",
      "it 484 - loss 3.9\n",
      "it 485 - loss 2.3\n",
      "it 486 - loss 4.9\n",
      "it 487 - loss 2.1\n",
      "it 488 - loss 4.2\n",
      "it 489 - loss 5.4\n",
      "it 490 - loss 4.8\n",
      "it 491 - loss 3.4\n",
      "it 492 - loss 4.6\n",
      "it 493 - loss 3.6\n",
      "it 494 - loss 1.9\n",
      "it 495 - loss 3.4\n",
      "it 496 - loss 3.6\n",
      "it 497 - loss 4.7\n",
      "it 498 - loss 5.2\n",
      "it 499 - loss 4.4\n",
      "it 500 - loss 4.8\n",
      "it 501 - loss 2.0\n",
      "it 502 - loss 4.8\n",
      "it 503 - loss 2.8\n",
      "it 504 - loss 2.6\n",
      "it 505 - loss 4.4\n",
      "it 506 - loss 3.9\n",
      "it 507 - loss 5.0\n",
      "it 508 - loss 1.8\n",
      "it 509 - loss 2.5\n",
      "it 510 - loss 3.8\n",
      "it 511 - loss 2.5\n",
      "it 512 - loss 3.3\n",
      "it 513 - loss 4.1\n",
      "it 514 - loss 2.6\n",
      "it 515 - loss 3.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 516 - loss 3.1\n",
      "it 517 - loss 3.1\n",
      "it 518 - loss 4.0\n",
      "it 519 - loss 3.5\n",
      "it 520 - loss 2.9\n",
      "it 521 - loss 3.0\n",
      "it 522 - loss 3.2\n",
      "it 523 - loss 2.8\n",
      "it 524 - loss 2.3\n",
      "it 525 - loss 2.9\n",
      "it 526 - loss 2.0\n",
      "it 527 - loss 1.7\n",
      "it 528 - loss 2.8\n",
      "it 529 - loss 4.1\n",
      "it 530 - loss 1.7\n",
      "it 531 - loss 4.4\n",
      "it 532 - loss 2.2\n",
      "it 533 - loss 3.2\n",
      "it 534 - loss 2.6\n",
      "it 535 - loss 2.8\n",
      "it 536 - loss 4.7\n",
      "it 537 - loss 0.6\n",
      "it 538 - loss 5.3\n",
      "it 539 - loss 2.3\n",
      "it 540 - loss 3.1\n",
      "it 541 - loss 2.7\n",
      "it 542 - loss 3.9\n",
      "it 543 - loss 3.3\n",
      "it 544 - loss 3.0\n",
      "it 545 - loss 4.8\n",
      "it 546 - loss 4.4\n",
      "it 547 - loss 4.3\n",
      "it 548 - loss 1.9\n",
      "it 549 - loss 6.0\n",
      "it 550 - loss 3.2\n",
      "it 551 - loss 1.6\n",
      "it 552 - loss 3.5\n",
      "it 553 - loss 2.6\n",
      "it 554 - loss 2.4\n",
      "it 555 - loss 3.2\n",
      "it 556 - loss 4.2\n",
      "it 557 - loss 3.0\n",
      "it 558 - loss 4.1\n",
      "it 559 - loss 3.0\n",
      "it 560 - loss 3.0\n",
      "it 561 - loss 2.5\n",
      "it 562 - loss 4.2\n",
      "it 563 - loss 4.1\n",
      "it 564 - loss 2.7\n",
      "it 565 - loss 3.4\n",
      "it 566 - loss 5.5\n",
      "it 567 - loss 0.9\n",
      "it 568 - loss 3.3\n",
      "it 569 - loss 2.4\n",
      "it 570 - loss 1.3\n",
      "it 571 - loss 3.5\n",
      "it 572 - loss 4.1\n",
      "it 573 - loss 4.8\n",
      "it 574 - loss 3.0\n",
      "it 575 - loss 2.5\n",
      "it 576 - loss 3.3\n",
      "it 577 - loss 2.9\n",
      "it 578 - loss 4.8\n",
      "it 579 - loss 3.7\n",
      "it 580 - loss 2.8\n",
      "it 581 - loss 3.6\n",
      "it 582 - loss 2.2\n",
      "it 583 - loss 1.5\n",
      "it 584 - loss 3.3\n",
      "it 585 - loss 4.8\n",
      "it 586 - loss 0.6\n",
      "it 587 - loss 2.8\n",
      "it 588 - loss 3.9\n",
      "it 589 - loss 4.2\n",
      "it 590 - loss 1.8\n",
      "it 591 - loss 3.3\n",
      "it 592 - loss 3.8\n",
      "it 593 - loss 3.4\n",
      "it 594 - loss 2.4\n",
      "it 595 - loss 3.0\n",
      "it 596 - loss 4.7\n",
      "it 597 - loss 3.5\n",
      "it 598 - loss 3.1\n",
      "it 599 - loss 4.6\n",
      "it 600 - loss 2.7\n",
      "it 601 - loss 3.6\n",
      "it 602 - loss 2.5\n",
      "it 603 - loss 1.3\n",
      "it 604 - loss 1.6\n",
      "it 605 - loss 3.2\n",
      "it 606 - loss 4.1\n",
      "it 607 - loss 3.3\n",
      "it 608 - loss 2.5\n",
      "it 609 - loss 1.3\n",
      "it 610 - loss 3.5\n",
      "it 611 - loss 4.9\n",
      "it 612 - loss 2.1\n",
      "it 613 - loss 2.6\n",
      "it 614 - loss 4.2\n",
      "it 615 - loss 3.0\n",
      "it 616 - loss 3.6\n",
      "it 617 - loss 3.7\n",
      "it 618 - loss 2.3\n",
      "it 619 - loss 2.4\n",
      "it 620 - loss 4.5\n",
      "it 621 - loss 3.9\n",
      "it 622 - loss 5.8\n",
      "it 623 - loss 2.7\n",
      "it 624 - loss 3.0\n",
      "it 625 - loss 2.7\n",
      "it 626 - loss 2.8\n",
      "it 627 - loss 1.3\n",
      "it 628 - loss 4.3\n",
      "it 629 - loss 3.6\n",
      "it 630 - loss 1.5\n",
      "it 631 - loss 2.1\n",
      "it 632 - loss 1.2\n",
      "it 633 - loss 2.6\n",
      "it 634 - loss 5.4\n",
      "it 635 - loss 2.5\n",
      "it 636 - loss 3.7\n",
      "it 637 - loss 4.2\n",
      "it 638 - loss 1.2\n",
      "it 639 - loss 1.9\n",
      "it 640 - loss 2.2\n",
      "it 641 - loss 4.4\n",
      "it 642 - loss 2.1\n",
      "it 643 - loss 3.1\n",
      "it 644 - loss 3.4\n",
      "it 645 - loss 1.7\n",
      "it 646 - loss 3.7\n",
      "it 647 - loss 3.0\n",
      "it 648 - loss 2.2\n",
      "it 649 - loss 1.8\n",
      "it 650 - loss 3.2\n",
      "it 651 - loss 2.2\n",
      "it 652 - loss 4.1\n",
      "it 653 - loss 3.7\n",
      "it 654 - loss 4.1\n",
      "it 655 - loss 1.5\n",
      "it 656 - loss 2.4\n",
      "it 657 - loss 3.9\n",
      "it 658 - loss 3.5\n",
      "it 659 - loss 3.7\n",
      "it 660 - loss 2.4\n",
      "it 661 - loss 3.6\n",
      "it 662 - loss 2.1\n",
      "it 663 - loss 3.1\n",
      "it 664 - loss 2.3\n",
      "it 665 - loss 1.1\n",
      "it 666 - loss 3.4\n",
      "it 667 - loss 3.2\n",
      "it 668 - loss 2.4\n",
      "it 669 - loss 1.1\n",
      "it 670 - loss 3.2\n",
      "it 671 - loss 1.7\n",
      "it 672 - loss 4.2\n",
      "it 673 - loss 1.2\n",
      "it 674 - loss 2.5\n",
      "it 675 - loss 2.4\n",
      "it 676 - loss 1.3\n",
      "it 677 - loss 2.6\n",
      "it 678 - loss 3.9\n",
      "it 679 - loss 2.8\n",
      "it 680 - loss 2.2\n",
      "it 681 - loss 2.6\n",
      "it 682 - loss 1.9\n",
      "it 683 - loss 2.7\n",
      "it 684 - loss 1.3\n",
      "it 685 - loss 2.3\n",
      "it 686 - loss 2.6\n",
      "it 687 - loss 3.3\n",
      "it 688 - loss 3.8\n",
      "it 689 - loss 2.3\n",
      "it 690 - loss 3.3\n",
      "it 691 - loss 1.9\n",
      "it 692 - loss 1.8\n",
      "it 693 - loss 2.1\n",
      "it 694 - loss 2.6\n",
      "it 695 - loss 1.9\n",
      "it 696 - loss 3.1\n",
      "it 697 - loss 1.8\n",
      "it 698 - loss 3.5\n",
      "it 699 - loss 1.7\n",
      "it 700 - loss 1.9\n",
      "it 701 - loss 2.2\n",
      "it 702 - loss 4.4\n",
      "it 703 - loss 1.5\n",
      "it 704 - loss 2.5\n",
      "it 705 - loss 1.3\n",
      "it 706 - loss 2.4\n",
      "it 707 - loss 1.8\n",
      "it 708 - loss 2.6\n",
      "it 709 - loss 3.2\n",
      "it 710 - loss 4.1\n",
      "it 711 - loss 2.8\n",
      "it 712 - loss 2.2\n",
      "it 713 - loss 1.5\n",
      "it 714 - loss 2.2\n",
      "it 715 - loss 1.4\n",
      "it 716 - loss 1.8\n",
      "it 717 - loss 2.5\n",
      "it 718 - loss 1.7\n",
      "it 719 - loss 1.4\n",
      "it 720 - loss 1.5\n",
      "it 721 - loss 2.0\n",
      "it 722 - loss 2.8\n",
      "it 723 - loss 1.7\n",
      "it 724 - loss 2.8\n",
      "it 725 - loss 2.5\n",
      "it 726 - loss 2.3\n",
      "it 727 - loss 3.2\n",
      "it 728 - loss 1.9\n",
      "it 729 - loss 2.4\n",
      "it 730 - loss 3.3\n",
      "it 731 - loss 2.3\n",
      "it 732 - loss 2.3\n",
      "it 733 - loss 2.2\n",
      "it 734 - loss 2.7\n",
      "it 735 - loss 1.5\n",
      "it 736 - loss 2.8\n",
      "it 737 - loss 1.5\n",
      "it 738 - loss 1.6\n",
      "it 739 - loss 2.9\n",
      "it 740 - loss 1.6\n",
      "it 741 - loss 3.9\n",
      "it 742 - loss 1.1\n",
      "it 743 - loss 2.3\n",
      "it 744 - loss 3.0\n",
      "it 745 - loss 2.1\n",
      "it 746 - loss 3.2\n",
      "it 747 - loss 1.8\n",
      "it 748 - loss 2.6\n",
      "it 749 - loss 3.1\n",
      "it 750 - loss 2.4\n",
      "it 751 - loss 1.7\n",
      "it 752 - loss 1.2\n",
      "it 753 - loss 1.1\n",
      "it 754 - loss 2.2\n",
      "it 755 - loss 2.2\n",
      "it 756 - loss 1.8\n",
      "it 757 - loss 2.9\n",
      "it 758 - loss 3.5\n",
      "it 759 - loss 2.8\n",
      "it 760 - loss 2.0\n",
      "it 761 - loss 6.2\n",
      "it 762 - loss 2.3\n",
      "it 763 - loss 2.4\n",
      "it 764 - loss 1.7\n",
      "it 765 - loss 1.5\n",
      "it 766 - loss 2.0\n",
      "it 767 - loss 2.8\n",
      "it 768 - loss 3.3\n",
      "it 769 - loss 0.8\n",
      "it 770 - loss 1.7\n",
      "it 771 - loss 3.3\n",
      "it 772 - loss 1.8\n",
      "it 773 - loss 0.8\n",
      "it 774 - loss 3.2\n",
      "it 775 - loss 4.6\n",
      "it 776 - loss 1.3\n",
      "it 777 - loss 1.9\n",
      "it 778 - loss 1.6\n",
      "it 779 - loss 1.8\n",
      "it 780 - loss 2.6\n",
      "it 781 - loss 1.6\n",
      "it 782 - loss 0.9\n",
      "it 783 - loss 3.0\n",
      "it 784 - loss 2.0\n",
      "it 785 - loss 3.2\n",
      "it 786 - loss 1.3\n",
      "it 787 - loss 1.5\n",
      "it 788 - loss 3.1\n",
      "it 789 - loss 2.5\n",
      "it 790 - loss 1.6\n",
      "it 791 - loss 1.2\n",
      "it 792 - loss 2.0\n",
      "it 793 - loss 2.6\n",
      "it 794 - loss 1.6\n",
      "it 795 - loss 3.5\n",
      "it 796 - loss 1.2\n",
      "it 797 - loss 1.1\n",
      "it 798 - loss 3.5\n",
      "it 799 - loss 2.1\n",
      "it 800 - loss 2.4\n",
      "it 801 - loss 1.9\n",
      "it 802 - loss 2.8\n",
      "it 803 - loss 2.6\n",
      "it 804 - loss 2.8\n",
      "it 805 - loss 2.9\n",
      "it 806 - loss 2.3\n",
      "it 807 - loss 1.8\n",
      "it 808 - loss 2.4\n",
      "it 809 - loss 0.9\n",
      "it 810 - loss 2.4\n",
      "it 811 - loss 2.0\n",
      "it 812 - loss 3.7\n",
      "it 813 - loss 1.8\n",
      "it 814 - loss 1.8\n",
      "it 815 - loss 2.4\n",
      "it 816 - loss 2.7\n",
      "it 817 - loss 1.7\n",
      "it 818 - loss 1.5\n",
      "it 819 - loss 1.4\n",
      "it 820 - loss 2.4\n",
      "it 821 - loss 1.7\n",
      "it 822 - loss 1.4\n",
      "it 823 - loss 1.7\n",
      "it 824 - loss 1.4\n",
      "it 825 - loss 1.8\n",
      "it 826 - loss 1.9\n",
      "it 827 - loss 3.3\n",
      "it 828 - loss 2.2\n",
      "it 829 - loss 2.6\n",
      "it 830 - loss 2.5\n",
      "it 831 - loss 2.1\n",
      "it 832 - loss 2.3\n",
      "it 833 - loss 2.3\n",
      "it 834 - loss 3.5\n",
      "it 835 - loss 2.1\n",
      "it 836 - loss 3.7\n",
      "it 837 - loss 1.2\n",
      "it 838 - loss 1.9\n",
      "it 839 - loss 1.6\n",
      "it 840 - loss 2.0\n",
      "it 841 - loss 1.5\n",
      "it 842 - loss 1.0\n",
      "it 843 - loss 2.9\n",
      "it 844 - loss 3.3\n",
      "it 845 - loss 1.3\n",
      "it 846 - loss 1.6\n",
      "it 847 - loss 3.1\n",
      "it 848 - loss 1.4\n",
      "it 849 - loss 2.4\n",
      "it 850 - loss 2.3\n",
      "it 851 - loss 1.0\n",
      "it 852 - loss 1.8\n",
      "it 853 - loss 1.8\n",
      "it 854 - loss 2.2\n",
      "it 855 - loss 2.0\n",
      "it 856 - loss 1.7\n",
      "it 857 - loss 1.1\n",
      "it 858 - loss 2.7\n",
      "it 859 - loss 2.9\n",
      "it 860 - loss 1.6\n",
      "it 861 - loss 1.6\n",
      "it 862 - loss 2.3\n",
      "it 863 - loss 1.7\n",
      "it 864 - loss 2.9\n",
      "it 865 - loss 2.4\n",
      "it 866 - loss 1.5\n",
      "it 867 - loss 1.7\n",
      "it 868 - loss 1.5\n",
      "it 869 - loss 3.6\n",
      "it 870 - loss 2.0\n",
      "it 871 - loss 2.5\n",
      "it 872 - loss 2.3\n",
      "it 873 - loss 2.4\n",
      "it 874 - loss 2.4\n",
      "it 875 - loss 2.0\n",
      "it 876 - loss 1.4\n",
      "it 877 - loss 1.5\n",
      "it 878 - loss 1.6\n",
      "it 879 - loss 2.7\n",
      "it 880 - loss 2.1\n",
      "it 881 - loss 2.6\n",
      "it 882 - loss 2.7\n",
      "it 883 - loss 2.0\n",
      "it 884 - loss 2.9\n",
      "it 885 - loss 1.5\n",
      "it 886 - loss 1.1\n",
      "it 887 - loss 2.9\n",
      "it 888 - loss 0.6\n",
      "it 889 - loss 2.4\n",
      "it 890 - loss 3.3\n",
      "it 891 - loss 1.9\n",
      "it 892 - loss 1.5\n",
      "it 893 - loss 2.2\n",
      "it 894 - loss 1.8\n",
      "it 895 - loss 2.3\n",
      "it 896 - loss 2.1\n",
      "it 897 - loss 1.8\n",
      "it 898 - loss 1.5\n",
      "it 899 - loss 1.3\n",
      "it 900 - loss 3.3\n",
      "it 901 - loss 1.4\n",
      "it 902 - loss 3.4\n",
      "it 903 - loss 3.8\n",
      "it 904 - loss 2.7\n",
      "it 905 - loss 2.1\n",
      "it 906 - loss 2.5\n",
      "it 907 - loss 1.3\n",
      "it 908 - loss 1.3\n",
      "it 909 - loss 2.3\n",
      "it 910 - loss 3.6\n",
      "it 911 - loss 2.6\n",
      "it 912 - loss 5.6\n",
      "it 913 - loss 3.4\n",
      "it 914 - loss 2.4\n",
      "it 915 - loss 1.5\n",
      "it 916 - loss 3.0\n",
      "it 917 - loss 2.2\n",
      "it 918 - loss 1.4\n",
      "it 919 - loss 1.7\n",
      "it 920 - loss 3.2\n",
      "it 921 - loss 3.6\n",
      "it 922 - loss 2.7\n",
      "it 923 - loss 2.6\n",
      "it 924 - loss 1.4\n",
      "it 925 - loss 1.4\n",
      "it 926 - loss 2.5\n",
      "it 927 - loss 2.2\n",
      "it 928 - loss 2.6\n",
      "it 929 - loss 2.2\n",
      "it 930 - loss 2.5\n",
      "it 931 - loss 1.6\n",
      "it 932 - loss 1.4\n",
      "it 933 - loss 2.5\n",
      "it 934 - loss 1.7\n",
      "it 935 - loss 1.7\n",
      "it 936 - loss 2.2\n",
      "it 937 - loss 1.8\n",
      "it 938 - loss 1.8\n",
      "it 939 - loss 2.0\n",
      "it 940 - loss 2.0\n",
      "it 941 - loss 1.7\n",
      "it 942 - loss 1.8\n",
      "it 943 - loss 0.9\n",
      "it 944 - loss 2.6\n",
      "it 945 - loss 1.9\n",
      "it 946 - loss 1.5\n",
      "it 947 - loss 1.7\n",
      "it 948 - loss 2.3\n",
      "it 949 - loss 2.2\n",
      "it 950 - loss 1.8\n",
      "it 951 - loss 1.7\n",
      "it 952 - loss 1.9\n",
      "it 953 - loss 2.3\n",
      "it 954 - loss 2.8\n",
      "it 955 - loss 2.0\n",
      "it 956 - loss 1.4\n",
      "it 957 - loss 1.5\n",
      "it 958 - loss 1.6\n",
      "it 959 - loss 1.1\n",
      "it 960 - loss 1.7\n",
      "it 961 - loss 2.1\n",
      "it 962 - loss 2.0\n",
      "it 963 - loss 3.3\n",
      "it 964 - loss 2.6\n",
      "it 965 - loss 4.0\n",
      "it 966 - loss 1.7\n",
      "it 967 - loss 2.4\n",
      "it 968 - loss 2.2\n",
      "it 969 - loss 4.5\n",
      "it 970 - loss 2.1\n",
      "it 971 - loss 1.7\n",
      "it 972 - loss 1.8\n",
      "it 973 - loss 1.0\n",
      "it 974 - loss 0.9\n",
      "it 975 - loss 1.8\n",
      "it 976 - loss 1.8\n",
      "it 977 - loss 1.0\n",
      "it 978 - loss 2.1\n",
      "it 979 - loss 1.4\n",
      "it 980 - loss 2.5\n",
      "it 981 - loss 1.6\n",
      "it 982 - loss 0.9\n",
      "it 983 - loss 1.7\n",
      "it 984 - loss 2.4\n",
      "it 985 - loss 0.9\n",
      "it 986 - loss 3.3\n",
      "it 987 - loss 1.7\n",
      "it 988 - loss 1.4\n",
      "it 989 - loss 0.9\n",
      "it 990 - loss 2.2\n",
      "it 991 - loss 1.7\n",
      "it 992 - loss 3.0\n",
      "it 993 - loss 1.0\n",
      "it 994 - loss 2.9\n",
      "it 995 - loss 2.5\n",
      "it 996 - loss 3.6\n",
      "it 997 - loss 1.7\n",
      "it 998 - loss 2.4\n",
      "it 999 - loss 0.9\n",
      "it 1000 - loss 2.6\n",
      "it 1001 - loss 3.0\n",
      "it 1002 - loss 1.2\n",
      "it 1003 - loss 0.9\n",
      "it 1004 - loss 1.4\n",
      "it 1005 - loss 2.0\n",
      "it 1006 - loss 1.8\n",
      "it 1007 - loss 0.6\n",
      "it 1008 - loss 1.6\n",
      "it 1009 - loss 2.1\n",
      "it 1010 - loss 2.6\n",
      "it 1011 - loss 2.5\n",
      "it 1012 - loss 1.4\n",
      "it 1013 - loss 1.4\n",
      "it 1014 - loss 1.3\n",
      "it 1015 - loss 1.8\n",
      "it 1016 - loss 1.3\n",
      "it 1017 - loss 1.9\n",
      "it 1018 - loss 2.0\n",
      "it 1019 - loss 1.9\n",
      "it 1020 - loss 1.2\n",
      "it 1021 - loss 1.6\n",
      "it 1022 - loss 1.8\n",
      "it 1023 - loss 1.4\n",
      "it 1024 - loss 1.9\n",
      "it 1025 - loss 1.9\n",
      "it 1026 - loss 1.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 1027 - loss 3.4\n",
      "it 1028 - loss 1.3\n",
      "it 1029 - loss 1.6\n",
      "it 1030 - loss 1.8\n",
      "it 1031 - loss 0.7\n",
      "it 1032 - loss 0.8\n",
      "it 1033 - loss 1.5\n",
      "it 1034 - loss 1.8\n",
      "it 1035 - loss 1.9\n",
      "it 1036 - loss 1.6\n",
      "it 1037 - loss 2.7\n",
      "it 1038 - loss 0.5\n",
      "it 1039 - loss 1.1\n",
      "it 1040 - loss 1.4\n",
      "it 1041 - loss 2.0\n",
      "it 1042 - loss 1.2\n",
      "it 1043 - loss 2.8\n",
      "it 1044 - loss 1.8\n",
      "it 1045 - loss 3.8\n",
      "it 1046 - loss 1.0\n",
      "it 1047 - loss 1.4\n",
      "it 1048 - loss 3.1\n",
      "it 1049 - loss 1.6\n",
      "it 1050 - loss 2.1\n",
      "it 1051 - loss 1.7\n",
      "it 1052 - loss 1.6\n",
      "it 1053 - loss 2.2\n",
      "it 1054 - loss 2.0\n",
      "it 1055 - loss 0.9\n",
      "it 1056 - loss 1.2\n",
      "it 1057 - loss 2.5\n",
      "it 1058 - loss 2.3\n",
      "it 1059 - loss 1.7\n",
      "it 1060 - loss 1.7\n",
      "it 1061 - loss 1.0\n",
      "it 1062 - loss 1.4\n",
      "it 1063 - loss 1.8\n",
      "it 1064 - loss 1.0\n",
      "it 1065 - loss 1.7\n",
      "it 1066 - loss 4.0\n",
      "it 1067 - loss 1.2\n",
      "it 1068 - loss 2.0\n",
      "it 1069 - loss 0.8\n",
      "it 1070 - loss 1.7\n",
      "it 1071 - loss 1.3\n",
      "it 1072 - loss 1.4\n",
      "it 1073 - loss 1.7\n",
      "it 1074 - loss 1.1\n",
      "it 1075 - loss 2.7\n",
      "it 1076 - loss 0.7\n",
      "it 1077 - loss 1.4\n",
      "it 1078 - loss 1.9\n",
      "it 1079 - loss 1.3\n",
      "it 1080 - loss 1.7\n",
      "it 1081 - loss 2.3\n",
      "it 1082 - loss 1.7\n",
      "it 1083 - loss 1.2\n",
      "it 1084 - loss 1.0\n",
      "it 1085 - loss 1.4\n",
      "it 1086 - loss 1.6\n",
      "it 1087 - loss 2.4\n",
      "it 1088 - loss 2.6\n",
      "it 1089 - loss 1.5\n",
      "it 1090 - loss 4.4\n",
      "it 1091 - loss 1.8\n",
      "it 1092 - loss 1.4\n",
      "it 1093 - loss 1.5\n",
      "it 1094 - loss 1.7\n",
      "it 1095 - loss 1.8\n",
      "it 1096 - loss 0.8\n",
      "it 1097 - loss 1.8\n",
      "it 1098 - loss 1.1\n",
      "it 1099 - loss 1.0\n",
      "it 1100 - loss 0.9\n",
      "it 1101 - loss 1.6\n",
      "it 1102 - loss 1.0\n",
      "it 1103 - loss 1.4\n",
      "it 1104 - loss 1.8\n",
      "it 1105 - loss 2.1\n",
      "it 1106 - loss 0.7\n",
      "it 1107 - loss 1.8\n",
      "it 1108 - loss 1.3\n",
      "it 1109 - loss 1.1\n",
      "it 1110 - loss 2.5\n",
      "it 1111 - loss 2.3\n",
      "it 1112 - loss 0.9\n",
      "it 1113 - loss 1.8\n",
      "it 1114 - loss 3.2\n",
      "it 1115 - loss 1.0\n",
      "it 1116 - loss 0.8\n",
      "it 1117 - loss 1.3\n",
      "it 1118 - loss 1.3\n",
      "it 1119 - loss 0.7\n",
      "it 1120 - loss 1.8\n",
      "it 1121 - loss 2.6\n",
      "it 1122 - loss 2.3\n",
      "it 1123 - loss 1.3\n",
      "it 1124 - loss 1.5\n",
      "it 1125 - loss 1.8\n",
      "it 1126 - loss 2.3\n",
      "it 1127 - loss 2.1\n",
      "it 1128 - loss 1.6\n",
      "it 1129 - loss 0.8\n",
      "it 1130 - loss 1.6\n",
      "it 1131 - loss 2.4\n",
      "it 1132 - loss 1.0\n",
      "it 1133 - loss 1.4\n",
      "it 1134 - loss 1.1\n",
      "it 1135 - loss 2.2\n",
      "it 1136 - loss 0.4\n",
      "it 1137 - loss 1.9\n",
      "it 1138 - loss 1.5\n",
      "it 1139 - loss 3.0\n",
      "it 1140 - loss 1.2\n",
      "it 1141 - loss 2.0\n",
      "it 1142 - loss 2.7\n",
      "it 1143 - loss 1.0\n",
      "it 1144 - loss 1.5\n",
      "it 1145 - loss 1.3\n",
      "it 1146 - loss 2.9\n",
      "it 1147 - loss 1.7\n",
      "it 1148 - loss 1.9\n",
      "it 1149 - loss 1.6\n",
      "it 1150 - loss 1.4\n",
      "it 1151 - loss 1.3\n",
      "it 1152 - loss 1.0\n",
      "it 1153 - loss 0.9\n",
      "it 1154 - loss 1.0\n",
      "it 1155 - loss 2.1\n",
      "it 1156 - loss 1.2\n",
      "it 1157 - loss 1.7\n",
      "it 1158 - loss 0.4\n",
      "it 1159 - loss 0.9\n",
      "it 1160 - loss 0.8\n",
      "it 1161 - loss 0.9\n",
      "it 1162 - loss 0.8\n",
      "it 1163 - loss 1.4\n",
      "it 1164 - loss 1.5\n",
      "it 1165 - loss 1.6\n",
      "it 1166 - loss 1.5\n",
      "it 1167 - loss 1.4\n",
      "it 1168 - loss 1.2\n",
      "it 1169 - loss 2.0\n",
      "it 1170 - loss 1.8\n",
      "it 1171 - loss 2.0\n",
      "it 1172 - loss 0.8\n",
      "it 1173 - loss 1.4\n",
      "it 1174 - loss 1.6\n",
      "it 1175 - loss 2.0\n",
      "it 1176 - loss 0.9\n",
      "it 1177 - loss 2.1\n",
      "it 1178 - loss 1.0\n",
      "it 1179 - loss 1.9\n",
      "it 1180 - loss 1.0\n",
      "it 1181 - loss 2.1\n",
      "it 1182 - loss 0.4\n",
      "it 1183 - loss 1.1\n",
      "it 1184 - loss 2.4\n",
      "it 1185 - loss 0.8\n",
      "it 1186 - loss 1.2\n",
      "it 1187 - loss 1.6\n",
      "it 1188 - loss 0.8\n",
      "it 1189 - loss 1.0\n",
      "it 1190 - loss 1.3\n",
      "it 1191 - loss 0.7\n",
      "it 1192 - loss 1.0\n",
      "it 1193 - loss 0.4\n",
      "it 1194 - loss 1.9\n",
      "it 1195 - loss 1.4\n",
      "it 1196 - loss 1.1\n",
      "it 1197 - loss 1.2\n",
      "it 1198 - loss 3.2\n",
      "it 1199 - loss 1.5\n",
      "it 1200 - loss 1.9\n",
      "it 1201 - loss 1.6\n",
      "it 1202 - loss 0.7\n",
      "it 1203 - loss 1.1\n",
      "it 1204 - loss 1.2\n",
      "it 1205 - loss 1.0\n",
      "it 1206 - loss 1.7\n",
      "it 1207 - loss 0.7\n",
      "it 1208 - loss 1.3\n",
      "it 1209 - loss 1.1\n",
      "it 1210 - loss 0.9\n",
      "it 1211 - loss 0.7\n",
      "it 1212 - loss 1.3\n",
      "it 1213 - loss 1.0\n",
      "it 1214 - loss 0.8\n",
      "it 1215 - loss 1.6\n",
      "it 1216 - loss 1.1\n",
      "it 1217 - loss 1.1\n",
      "it 1218 - loss 2.4\n",
      "it 1219 - loss 1.6\n",
      "it 1220 - loss 1.1\n",
      "it 1221 - loss 0.9\n",
      "it 1222 - loss 1.2\n",
      "it 1223 - loss 1.7\n",
      "it 1224 - loss 1.7\n",
      "it 1225 - loss 1.2\n",
      "it 1226 - loss 1.4\n",
      "it 1227 - loss 2.1\n",
      "it 1228 - loss 0.8\n",
      "it 1229 - loss 1.0\n",
      "it 1230 - loss 1.0\n",
      "it 1231 - loss 1.7\n",
      "it 1232 - loss 2.4\n",
      "it 1233 - loss 2.6\n",
      "it 1234 - loss 0.5\n",
      "it 1235 - loss 0.8\n",
      "it 1236 - loss 1.5\n",
      "it 1237 - loss 2.3\n",
      "it 1238 - loss 1.3\n",
      "it 1239 - loss 1.8\n",
      "it 1240 - loss 1.4\n",
      "it 1241 - loss 0.9\n",
      "it 1242 - loss 1.7\n",
      "it 1243 - loss 1.4\n",
      "it 1244 - loss 0.5\n",
      "it 1245 - loss 0.7\n",
      "it 1246 - loss 1.5\n",
      "it 1247 - loss 0.4\n",
      "it 1248 - loss 0.8\n",
      "it 1249 - loss 1.8\n",
      "it 1250 - loss 1.8\n",
      "it 1251 - loss 2.0\n",
      "it 1252 - loss 1.2\n",
      "it 1253 - loss 1.5\n",
      "it 1254 - loss 1.1\n",
      "it 1255 - loss 1.4\n",
      "it 1256 - loss 2.1\n",
      "it 1257 - loss 1.9\n",
      "it 1258 - loss 1.3\n",
      "it 1259 - loss 0.5\n",
      "it 1260 - loss 1.2\n",
      "it 1261 - loss 1.4\n",
      "it 1262 - loss 1.8\n",
      "it 1263 - loss 0.8\n",
      "it 1264 - loss 1.3\n",
      "it 1265 - loss 0.6\n",
      "it 1266 - loss 2.4\n",
      "it 1267 - loss 0.7\n",
      "it 1268 - loss 1.1\n",
      "it 1269 - loss 2.4\n",
      "it 1270 - loss 1.9\n",
      "it 1271 - loss 1.0\n",
      "it 1272 - loss 1.4\n",
      "it 1273 - loss 0.4\n",
      "it 1274 - loss 3.0\n",
      "it 1275 - loss 1.5\n",
      "it 1276 - loss 1.2\n",
      "it 1277 - loss 1.5\n",
      "it 1278 - loss 0.8\n",
      "it 1279 - loss 0.8\n",
      "it 1280 - loss 1.1\n",
      "it 1281 - loss 1.9\n",
      "it 1282 - loss 1.1\n",
      "it 1283 - loss 2.1\n",
      "it 1284 - loss 1.2\n",
      "it 1285 - loss 1.9\n",
      "it 1286 - loss 1.4\n",
      "it 1287 - loss 0.6\n",
      "it 1288 - loss 1.0\n",
      "it 1289 - loss 1.4\n",
      "it 1290 - loss 2.2\n",
      "it 1291 - loss 1.3\n",
      "it 1292 - loss 1.6\n",
      "it 1293 - loss 0.8\n",
      "it 1294 - loss 0.8\n",
      "it 1295 - loss 1.0\n",
      "it 1296 - loss 1.5\n",
      "it 1297 - loss 1.4\n",
      "it 1298 - loss 1.2\n",
      "it 1299 - loss 1.0\n",
      "it 1300 - loss 1.0\n",
      "it 1301 - loss 2.7\n",
      "it 1302 - loss 1.6\n",
      "it 1303 - loss 1.7\n",
      "it 1304 - loss 2.3\n",
      "it 1305 - loss 0.6\n",
      "it 1306 - loss 0.8\n",
      "it 1307 - loss 1.4\n",
      "it 1308 - loss 1.5\n",
      "it 1309 - loss 1.6\n",
      "it 1310 - loss 1.6\n",
      "it 1311 - loss 2.4\n",
      "it 1312 - loss 1.2\n",
      "it 1313 - loss 0.9\n",
      "it 1314 - loss 1.4\n",
      "it 1315 - loss 1.1\n",
      "it 1316 - loss 1.3\n",
      "it 1317 - loss 0.7\n",
      "it 1318 - loss 1.6\n",
      "it 1319 - loss 1.0\n",
      "it 1320 - loss 0.5\n",
      "it 1321 - loss 1.1\n",
      "it 1322 - loss 1.1\n",
      "it 1323 - loss 0.6\n",
      "it 1324 - loss 1.9\n",
      "it 1325 - loss 1.0\n",
      "it 1326 - loss 1.4\n",
      "it 1327 - loss 1.6\n",
      "it 1328 - loss 1.3\n",
      "it 1329 - loss 1.7\n",
      "it 1330 - loss 1.3\n",
      "it 1331 - loss 1.6\n",
      "it 1332 - loss 1.1\n",
      "it 1333 - loss 1.1\n",
      "it 1334 - loss 0.9\n",
      "it 1335 - loss 1.3\n",
      "it 1336 - loss 0.5\n",
      "it 1337 - loss 1.5\n",
      "it 1338 - loss 1.6\n",
      "it 1339 - loss 0.8\n",
      "it 1340 - loss 1.5\n",
      "it 1341 - loss 1.3\n",
      "it 1342 - loss 1.8\n",
      "it 1343 - loss 1.9\n",
      "it 1344 - loss 1.3\n",
      "it 1345 - loss 1.4\n",
      "it 1346 - loss 0.9\n",
      "it 1347 - loss 1.4\n",
      "it 1348 - loss 1.7\n",
      "it 1349 - loss 1.2\n",
      "it 1350 - loss 1.2\n",
      "it 1351 - loss 1.2\n",
      "it 1352 - loss 0.9\n",
      "it 1353 - loss 1.0\n",
      "it 1354 - loss 1.4\n",
      "it 1355 - loss 1.3\n",
      "it 1356 - loss 1.2\n",
      "it 1357 - loss 1.3\n",
      "it 1358 - loss 1.2\n",
      "it 1359 - loss 1.9\n",
      "it 1360 - loss 0.8\n",
      "it 1361 - loss 2.2\n",
      "it 1362 - loss 0.9\n",
      "it 1363 - loss 0.8\n",
      "it 1364 - loss 1.2\n",
      "it 1365 - loss 0.8\n",
      "it 1366 - loss 0.9\n",
      "it 1367 - loss 0.9\n",
      "it 1368 - loss 0.8\n",
      "it 1369 - loss 2.0\n",
      "it 1370 - loss 1.0\n",
      "it 1371 - loss 1.0\n",
      "it 1372 - loss 1.1\n",
      "it 1373 - loss 1.8\n",
      "it 1374 - loss 1.6\n",
      "it 1375 - loss 1.2\n",
      "it 1376 - loss 0.8\n",
      "it 1377 - loss 1.8\n",
      "it 1378 - loss 1.2\n",
      "it 1379 - loss 1.6\n",
      "it 1380 - loss 0.5\n",
      "it 1381 - loss 0.8\n",
      "it 1382 - loss 1.4\n",
      "it 1383 - loss 1.0\n",
      "it 1384 - loss 0.5\n",
      "it 1385 - loss 1.5\n",
      "it 1386 - loss 0.9\n",
      "it 1387 - loss 1.1\n",
      "it 1388 - loss 2.4\n",
      "it 1389 - loss 0.7\n",
      "it 1390 - loss 1.1\n",
      "it 1391 - loss 0.8\n",
      "it 1392 - loss 1.5\n",
      "it 1393 - loss 1.6\n",
      "it 1394 - loss 1.2\n",
      "it 1395 - loss 1.0\n",
      "it 1396 - loss 1.1\n",
      "it 1397 - loss 0.7\n",
      "it 1398 - loss 0.9\n",
      "it 1399 - loss 0.5\n",
      "it 1400 - loss 1.5\n",
      "it 1401 - loss 2.1\n",
      "it 1402 - loss 1.0\n",
      "it 1403 - loss 1.9\n",
      "it 1404 - loss 2.6\n",
      "it 1405 - loss 0.6\n",
      "it 1406 - loss 1.2\n",
      "it 1407 - loss 0.5\n",
      "it 1408 - loss 0.1\n",
      "it 1409 - loss 0.8\n",
      "it 1410 - loss 0.8\n",
      "it 1411 - loss 1.0\n",
      "it 1412 - loss 2.1\n",
      "it 1413 - loss 1.7\n",
      "it 1414 - loss 0.5\n",
      "it 1415 - loss 1.1\n",
      "it 1416 - loss 0.9\n",
      "it 1417 - loss 1.4\n",
      "it 1418 - loss 0.3\n",
      "it 1419 - loss 1.4\n",
      "it 1420 - loss 1.7\n",
      "it 1421 - loss 1.5\n",
      "it 1422 - loss 0.4\n",
      "it 1423 - loss 1.6\n",
      "it 1424 - loss 1.3\n",
      "it 1425 - loss 1.0\n",
      "it 1426 - loss 1.8\n",
      "it 1427 - loss 2.1\n",
      "it 1428 - loss 0.5\n",
      "it 1429 - loss 2.0\n",
      "it 1430 - loss 0.5\n",
      "it 1431 - loss 1.8\n",
      "it 1432 - loss 1.4\n",
      "it 1433 - loss 1.2\n",
      "it 1434 - loss 0.9\n",
      "it 1435 - loss 0.7\n",
      "it 1436 - loss 0.8\n",
      "it 1437 - loss 1.0\n",
      "it 1438 - loss 0.8\n",
      "it 1439 - loss 0.2\n",
      "it 1440 - loss 0.5\n",
      "it 1441 - loss 1.9\n",
      "it 1442 - loss 0.8\n",
      "it 1443 - loss 1.2\n",
      "it 1444 - loss 1.7\n",
      "it 1445 - loss 0.8\n",
      "it 1446 - loss 0.4\n",
      "it 1447 - loss 1.0\n",
      "it 1448 - loss 0.9\n",
      "it 1449 - loss 1.2\n",
      "it 1450 - loss 1.2\n",
      "it 1451 - loss 1.5\n",
      "it 1452 - loss 1.4\n",
      "it 1453 - loss 1.1\n",
      "it 1454 - loss 1.9\n",
      "it 1455 - loss 0.5\n",
      "it 1456 - loss 0.7\n",
      "it 1457 - loss 0.8\n",
      "it 1458 - loss 0.6\n",
      "it 1459 - loss 1.2\n",
      "it 1460 - loss 2.1\n",
      "it 1461 - loss 1.1\n",
      "it 1462 - loss 1.3\n",
      "it 1463 - loss 1.6\n",
      "it 1464 - loss 0.7\n",
      "it 1465 - loss 1.4\n",
      "it 1466 - loss 1.8\n",
      "it 1467 - loss 1.9\n",
      "it 1468 - loss 0.6\n",
      "it 1469 - loss 1.0\n",
      "it 1470 - loss 0.3\n",
      "it 1471 - loss 0.8\n",
      "it 1472 - loss 0.6\n",
      "it 1473 - loss 1.1\n",
      "it 1474 - loss 1.7\n",
      "it 1475 - loss 0.3\n",
      "it 1476 - loss 0.9\n",
      "it 1477 - loss 1.2\n",
      "it 1478 - loss 0.9\n",
      "it 1479 - loss 1.1\n",
      "it 1480 - loss 1.3\n",
      "it 1481 - loss 1.2\n",
      "it 1482 - loss 2.2\n",
      "it 1483 - loss 0.8\n",
      "it 1484 - loss 1.0\n",
      "it 1485 - loss 1.0\n",
      "it 1486 - loss 1.3\n",
      "it 1487 - loss 0.6\n",
      "it 1488 - loss 1.9\n",
      "it 1489 - loss 2.0\n",
      "it 1490 - loss 0.9\n",
      "it 1491 - loss 1.2\n",
      "it 1492 - loss 1.4\n",
      "it 1493 - loss 1.1\n",
      "it 1494 - loss 0.4\n",
      "it 1495 - loss 0.3\n",
      "it 1496 - loss 0.5\n",
      "it 1497 - loss 0.6\n",
      "it 1498 - loss 1.0\n",
      "it 1499 - loss 1.5\n",
      "it 1500 - loss 1.4\n",
      "it 1501 - loss 1.1\n",
      "it 1502 - loss 1.0\n",
      "it 1503 - loss 0.9\n",
      "it 1504 - loss 1.2\n",
      "it 1505 - loss 0.9\n",
      "it 1506 - loss 1.7\n",
      "it 1507 - loss 0.4\n",
      "it 1508 - loss 0.5\n",
      "it 1509 - loss 0.9\n",
      "it 1510 - loss 1.7\n",
      "it 1511 - loss 2.3\n",
      "it 1512 - loss 0.8\n",
      "it 1513 - loss 1.7\n",
      "it 1514 - loss 1.2\n",
      "it 1515 - loss 1.1\n",
      "it 1516 - loss 0.2\n",
      "it 1517 - loss 1.3\n",
      "it 1518 - loss 1.4\n",
      "it 1519 - loss 0.8\n",
      "it 1520 - loss 0.7\n",
      "it 1521 - loss 0.7\n",
      "it 1522 - loss 0.6\n",
      "it 1523 - loss 1.2\n",
      "it 1524 - loss 0.8\n",
      "it 1525 - loss 1.6\n",
      "it 1526 - loss 1.7\n",
      "it 1527 - loss 0.9\n",
      "it 1528 - loss 0.8\n",
      "it 1529 - loss 1.7\n",
      "it 1530 - loss 1.4\n",
      "it 1531 - loss 1.2\n",
      "it 1532 - loss 0.9\n",
      "it 1533 - loss 0.9\n",
      "it 1534 - loss 0.8\n",
      "it 1535 - loss 0.7\n",
      "it 1536 - loss 0.8\n",
      "it 1537 - loss 1.5\n",
      "it 1538 - loss 0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 1539 - loss 1.2\n",
      "it 1540 - loss 0.8\n",
      "it 1541 - loss 1.2\n",
      "it 1542 - loss 1.6\n",
      "it 1543 - loss 1.4\n",
      "it 1544 - loss 0.8\n",
      "it 1545 - loss 1.4\n",
      "it 1546 - loss 0.9\n",
      "it 1547 - loss 0.4\n",
      "it 1548 - loss 1.1\n",
      "it 1549 - loss 0.5\n",
      "it 1550 - loss 1.1\n",
      "it 1551 - loss 0.4\n",
      "it 1552 - loss 1.5\n",
      "it 1553 - loss 1.1\n",
      "it 1554 - loss 0.4\n",
      "it 1555 - loss 0.7\n",
      "it 1556 - loss 2.1\n",
      "it 1557 - loss 0.4\n",
      "it 1558 - loss 1.1\n",
      "it 1559 - loss 0.4\n",
      "it 1560 - loss 1.0\n",
      "it 1561 - loss 0.6\n",
      "it 1562 - loss 1.0\n",
      "it 1563 - loss 0.9\n",
      "it 1564 - loss 1.4\n",
      "it 1565 - loss 0.6\n",
      "it 1566 - loss 0.6\n",
      "it 1567 - loss 1.2\n",
      "it 1568 - loss 1.8\n",
      "it 1569 - loss 1.0\n",
      "it 1570 - loss 1.1\n",
      "it 1571 - loss 1.0\n",
      "it 1572 - loss 1.2\n",
      "it 1573 - loss 1.4\n",
      "it 1574 - loss 1.5\n",
      "it 1575 - loss 0.5\n",
      "it 1576 - loss 1.1\n",
      "it 1577 - loss 1.3\n",
      "it 1578 - loss 1.0\n",
      "it 1579 - loss 1.0\n",
      "it 1580 - loss 0.8\n",
      "it 1581 - loss 1.3\n",
      "it 1582 - loss 0.3\n",
      "it 1583 - loss 1.0\n",
      "it 1584 - loss 1.8\n",
      "it 1585 - loss 1.6\n",
      "it 1586 - loss 0.8\n",
      "it 1587 - loss 0.8\n",
      "it 1588 - loss 0.9\n",
      "it 1589 - loss 0.3\n",
      "it 1590 - loss 0.1\n",
      "it 1591 - loss 1.4\n",
      "it 1592 - loss 1.7\n",
      "it 1593 - loss 0.5\n",
      "it 1594 - loss 0.8\n",
      "it 1595 - loss 1.1\n",
      "it 1596 - loss 0.8\n",
      "it 1597 - loss 1.0\n",
      "it 1598 - loss 0.8\n",
      "it 1599 - loss 0.8\n",
      "it 1600 - loss 0.8\n",
      "it 1601 - loss 1.3\n",
      "it 1602 - loss 0.7\n",
      "it 1603 - loss 1.1\n",
      "it 1604 - loss 2.0\n",
      "it 1605 - loss 1.2\n",
      "it 1606 - loss 0.3\n",
      "it 1607 - loss 0.7\n",
      "it 1608 - loss 1.4\n",
      "it 1609 - loss 0.7\n",
      "it 1610 - loss 1.1\n",
      "it 1611 - loss 0.3\n",
      "it 1612 - loss 0.9\n",
      "it 1613 - loss 0.5\n",
      "it 1614 - loss 2.2\n",
      "it 1615 - loss 0.8\n",
      "it 1616 - loss 1.0\n",
      "it 1617 - loss 1.9\n",
      "it 1618 - loss 1.1\n",
      "it 1619 - loss 0.4\n",
      "it 1620 - loss 0.6\n",
      "it 1621 - loss 1.3\n",
      "it 1622 - loss 0.9\n",
      "it 1623 - loss 0.6\n",
      "it 1624 - loss 0.9\n",
      "it 1625 - loss 0.5\n",
      "it 1626 - loss 1.7\n",
      "it 1627 - loss 1.2\n",
      "it 1628 - loss 0.5\n",
      "it 1629 - loss 1.7\n",
      "it 1630 - loss 0.5\n",
      "it 1631 - loss 1.5\n",
      "it 1632 - loss 1.8\n",
      "it 1633 - loss 0.7\n",
      "it 1634 - loss 1.5\n",
      "it 1635 - loss 0.7\n",
      "it 1636 - loss 0.7\n",
      "it 1637 - loss 1.2\n",
      "it 1638 - loss 1.3\n",
      "it 1639 - loss 0.9\n",
      "it 1640 - loss 1.5\n",
      "it 1641 - loss 0.4\n",
      "it 1642 - loss 1.9\n",
      "it 1643 - loss 1.0\n",
      "it 1644 - loss 1.8\n",
      "it 1645 - loss 0.7\n",
      "it 1646 - loss 0.6\n",
      "it 1647 - loss 0.9\n",
      "it 1648 - loss 1.5\n",
      "it 1649 - loss 0.8\n",
      "it 1650 - loss 1.2\n",
      "it 1651 - loss 0.6\n",
      "it 1652 - loss 0.5\n",
      "it 1653 - loss 1.0\n",
      "it 1654 - loss 0.6\n",
      "it 1655 - loss 0.7\n",
      "it 1656 - loss 2.0\n",
      "it 1657 - loss 1.4\n",
      "it 1658 - loss 1.0\n",
      "it 1659 - loss 1.0\n",
      "it 1660 - loss 0.5\n",
      "it 1661 - loss 0.8\n",
      "it 1662 - loss 0.6\n",
      "it 1663 - loss 0.9\n",
      "it 1664 - loss 0.5\n",
      "it 1665 - loss 0.7\n",
      "it 1666 - loss 0.8\n",
      "it 1667 - loss 0.8\n",
      "it 1668 - loss 1.1\n",
      "it 1669 - loss 0.1\n",
      "it 1670 - loss 1.2\n",
      "it 1671 - loss 0.5\n",
      "it 1672 - loss 1.2\n",
      "it 1673 - loss 1.8\n",
      "it 1674 - loss 0.8\n",
      "it 1675 - loss 1.7\n",
      "it 1676 - loss 0.6\n",
      "it 1677 - loss 2.1\n",
      "it 1678 - loss 0.1\n",
      "it 1679 - loss 0.7\n",
      "it 1680 - loss 0.8\n",
      "it 1681 - loss 1.1\n",
      "it 1682 - loss 0.7\n",
      "it 1683 - loss 0.8\n",
      "it 1684 - loss 0.6\n",
      "it 1685 - loss 0.2\n",
      "it 1686 - loss 1.0\n",
      "it 1687 - loss 1.3\n",
      "it 1688 - loss 0.8\n",
      "it 1689 - loss 2.2\n",
      "it 1690 - loss 1.1\n",
      "it 1691 - loss 0.3\n",
      "it 1692 - loss 1.1\n",
      "it 1693 - loss 0.8\n",
      "it 1694 - loss 0.7\n",
      "it 1695 - loss 1.1\n",
      "it 1696 - loss 0.7\n",
      "it 1697 - loss 1.1\n",
      "it 1698 - loss 0.8\n",
      "it 1699 - loss 0.5\n",
      "it 1700 - loss 1.3\n",
      "it 1701 - loss 1.1\n",
      "it 1702 - loss 1.9\n",
      "it 1703 - loss 1.5\n",
      "it 1704 - loss 1.0\n",
      "it 1705 - loss 0.6\n",
      "it 1706 - loss 0.3\n",
      "it 1707 - loss 1.0\n",
      "it 1708 - loss 0.8\n",
      "it 1709 - loss 0.8\n",
      "it 1710 - loss 1.2\n",
      "it 1711 - loss 0.5\n",
      "it 1712 - loss 0.4\n",
      "it 1713 - loss 1.2\n",
      "it 1714 - loss 0.8\n",
      "it 1715 - loss 1.0\n",
      "it 1716 - loss 0.8\n",
      "it 1717 - loss 0.9\n",
      "it 1718 - loss 1.8\n",
      "it 1719 - loss 2.1\n",
      "it 1720 - loss 1.1\n",
      "it 1721 - loss 0.5\n",
      "it 1722 - loss 1.3\n",
      "it 1723 - loss 1.0\n",
      "it 1724 - loss 1.6\n",
      "it 1725 - loss 1.2\n",
      "it 1726 - loss 0.7\n",
      "it 1727 - loss 0.6\n",
      "it 1728 - loss 0.5\n",
      "it 1729 - loss 0.8\n",
      "it 1730 - loss 1.1\n",
      "it 1731 - loss 1.7\n",
      "it 1732 - loss 0.4\n",
      "it 1733 - loss 1.9\n",
      "it 1734 - loss 1.1\n",
      "it 1735 - loss 1.2\n",
      "it 1736 - loss 0.7\n",
      "it 1737 - loss 0.9\n",
      "it 1738 - loss 0.8\n",
      "it 1739 - loss 1.2\n",
      "it 1740 - loss 1.2\n",
      "it 1741 - loss 0.9\n",
      "it 1742 - loss 1.0\n",
      "it 1743 - loss 0.6\n",
      "it 1744 - loss 0.5\n",
      "it 1745 - loss 0.3\n",
      "it 1746 - loss 0.6\n",
      "it 1747 - loss 0.4\n",
      "it 1748 - loss 0.6\n",
      "it 1749 - loss 0.1\n",
      "it 1750 - loss 1.2\n",
      "it 1751 - loss 0.3\n",
      "it 1752 - loss 1.0\n",
      "it 1753 - loss 1.1\n",
      "it 1754 - loss 0.9\n",
      "it 1755 - loss 1.1\n",
      "it 1756 - loss 0.8\n",
      "it 1757 - loss 0.9\n",
      "it 1758 - loss 0.6\n",
      "it 1759 - loss 0.4\n",
      "it 1760 - loss 0.1\n",
      "it 1761 - loss 1.7\n",
      "it 1762 - loss 1.0\n",
      "it 1763 - loss 0.6\n",
      "it 1764 - loss 1.2\n",
      "it 1765 - loss 1.0\n",
      "it 1766 - loss 0.9\n",
      "it 1767 - loss 1.1\n",
      "it 1768 - loss 0.9\n",
      "it 1769 - loss 0.4\n",
      "it 1770 - loss 0.9\n",
      "it 1771 - loss 0.8\n",
      "it 1772 - loss 0.5\n",
      "it 1773 - loss 1.1\n",
      "it 1774 - loss 0.6\n",
      "it 1775 - loss 0.5\n",
      "it 1776 - loss 1.3\n",
      "it 1777 - loss 0.6\n",
      "it 1778 - loss 1.4\n",
      "it 1779 - loss 0.7\n",
      "it 1780 - loss 1.4\n",
      "it 1781 - loss 1.1\n",
      "it 1782 - loss 0.6\n",
      "it 1783 - loss 0.5\n",
      "it 1784 - loss 0.6\n",
      "it 1785 - loss 0.5\n",
      "it 1786 - loss 0.7\n",
      "it 1787 - loss 0.5\n",
      "it 1788 - loss 0.7\n",
      "it 1789 - loss 0.7\n",
      "it 1790 - loss 1.4\n",
      "it 1791 - loss 0.5\n",
      "it 1792 - loss 1.0\n",
      "it 1793 - loss 1.4\n",
      "it 1794 - loss 1.6\n",
      "it 1795 - loss 1.8\n",
      "it 1796 - loss 1.6\n",
      "it 1797 - loss 0.3\n",
      "it 1798 - loss 0.8\n",
      "it 1799 - loss 0.7\n",
      "it 1800 - loss 0.3\n",
      "it 1801 - loss 1.1\n",
      "it 1802 - loss 1.2\n",
      "it 1803 - loss 1.4\n",
      "it 1804 - loss 1.0\n",
      "it 1805 - loss 1.0\n",
      "it 1806 - loss 0.9\n",
      "it 1807 - loss 0.5\n",
      "it 1808 - loss 1.1\n",
      "it 1809 - loss 0.7\n",
      "it 1810 - loss 1.1\n",
      "it 1811 - loss 1.1\n",
      "it 1812 - loss 0.6\n",
      "it 1813 - loss 1.8\n",
      "it 1814 - loss 0.5\n",
      "it 1815 - loss 1.0\n",
      "it 1816 - loss 1.2\n",
      "it 1817 - loss 0.8\n",
      "it 1818 - loss 1.0\n",
      "it 1819 - loss 1.2\n",
      "it 1820 - loss 1.2\n",
      "it 1821 - loss 0.8\n",
      "it 1822 - loss 0.8\n",
      "it 1823 - loss 1.0\n",
      "it 1824 - loss 0.2\n",
      "it 1825 - loss 1.1\n",
      "it 1826 - loss 1.3\n",
      "it 1827 - loss 1.3\n",
      "it 1828 - loss 0.7\n",
      "it 1829 - loss 0.7\n",
      "it 1830 - loss 0.6\n",
      "it 1831 - loss 1.8\n",
      "it 1832 - loss 1.0\n",
      "it 1833 - loss 0.7\n",
      "it 1834 - loss 1.4\n",
      "it 1835 - loss 1.7\n",
      "it 1836 - loss 0.8\n",
      "it 1837 - loss 0.8\n",
      "it 1838 - loss 0.3\n",
      "it 1839 - loss 0.8\n",
      "it 1840 - loss 1.2\n",
      "it 1841 - loss 1.7\n",
      "it 1842 - loss 0.2\n",
      "it 1843 - loss 0.9\n",
      "it 1844 - loss 0.7\n",
      "it 1845 - loss 0.8\n",
      "it 1846 - loss 0.8\n",
      "it 1847 - loss 0.4\n",
      "it 1848 - loss 1.0\n",
      "it 1849 - loss 0.8\n",
      "it 1850 - loss 1.2\n",
      "it 1851 - loss 0.5\n",
      "it 1852 - loss 1.8\n",
      "it 1853 - loss 0.6\n",
      "it 1854 - loss 0.9\n",
      "it 1855 - loss 0.5\n",
      "it 1856 - loss 0.2\n",
      "it 1857 - loss 0.5\n",
      "it 1858 - loss 0.6\n",
      "it 1859 - loss 0.4\n",
      "it 1860 - loss 1.3\n",
      "it 1861 - loss 0.9\n",
      "it 1862 - loss 0.6\n",
      "it 1863 - loss 0.9\n",
      "it 1864 - loss 0.7\n",
      "it 1865 - loss 0.4\n",
      "it 1866 - loss 0.5\n",
      "it 1867 - loss 0.8\n",
      "it 1868 - loss 0.6\n",
      "it 1869 - loss 1.1\n",
      "it 1870 - loss 0.0\n",
      "it 1871 - loss 1.1\n",
      "it 1872 - loss 1.2\n",
      "it 1873 - loss 0.3\n",
      "it 1874 - loss 0.9\n",
      "it 1875 - loss 1.4\n",
      "it 1876 - loss 0.6\n",
      "it 1877 - loss 0.4\n",
      "it 1878 - loss 1.2\n",
      "it 1879 - loss 1.0\n",
      "it 1880 - loss 0.4\n",
      "it 1881 - loss 1.6\n",
      "it 1882 - loss 0.8\n",
      "it 1883 - loss 0.6\n",
      "it 1884 - loss 0.9\n",
      "it 1885 - loss 0.5\n",
      "it 1886 - loss 0.2\n",
      "it 1887 - loss 0.6\n",
      "it 1888 - loss 1.5\n",
      "it 1889 - loss 1.0\n",
      "it 1890 - loss 0.6\n",
      "it 1891 - loss 1.1\n",
      "it 1892 - loss 0.4\n",
      "it 1893 - loss 0.7\n",
      "it 1894 - loss 1.1\n",
      "it 1895 - loss 1.2\n",
      "it 1896 - loss 1.3\n",
      "it 1897 - loss 1.3\n",
      "it 1898 - loss 1.2\n",
      "it 1899 - loss 1.3\n",
      "it 1900 - loss 0.5\n",
      "it 1901 - loss 1.1\n",
      "it 1902 - loss 0.8\n",
      "it 1903 - loss 0.7\n",
      "it 1904 - loss 0.6\n",
      "it 1905 - loss 0.3\n",
      "it 1906 - loss 1.1\n",
      "it 1907 - loss 1.0\n",
      "it 1908 - loss 0.7\n",
      "it 1909 - loss 0.2\n",
      "it 1910 - loss 0.4\n",
      "it 1911 - loss 0.9\n",
      "it 1912 - loss 0.5\n",
      "it 1913 - loss 0.5\n",
      "it 1914 - loss 1.3\n",
      "it 1915 - loss 0.7\n",
      "it 1916 - loss 1.1\n",
      "it 1917 - loss 0.5\n",
      "it 1918 - loss 0.6\n",
      "it 1919 - loss 0.7\n",
      "it 1920 - loss 1.0\n",
      "it 1921 - loss 1.3\n",
      "it 1922 - loss 0.7\n",
      "it 1923 - loss 0.6\n",
      "it 1924 - loss 0.6\n",
      "it 1925 - loss 0.4\n",
      "it 1926 - loss 0.8\n",
      "it 1927 - loss 0.9\n",
      "it 1928 - loss 0.7\n",
      "it 1929 - loss 1.3\n",
      "it 1930 - loss 0.8\n",
      "it 1931 - loss 0.1\n",
      "it 1932 - loss 1.2\n",
      "it 1933 - loss 0.9\n",
      "it 1934 - loss 2.4\n",
      "it 1935 - loss 0.4\n",
      "it 1936 - loss 1.4\n",
      "it 1937 - loss 1.0\n",
      "it 1938 - loss 0.3\n",
      "it 1939 - loss 1.2\n",
      "it 1940 - loss 1.8\n",
      "it 1941 - loss 0.6\n",
      "it 1942 - loss 1.1\n",
      "it 1943 - loss 1.0\n",
      "it 1944 - loss 1.9\n",
      "it 1945 - loss 1.5\n",
      "it 1946 - loss 1.2\n",
      "it 1947 - loss 0.3\n",
      "it 1948 - loss 0.6\n",
      "it 1949 - loss 0.7\n",
      "it 1950 - loss 1.1\n",
      "it 1951 - loss 0.9\n",
      "it 1952 - loss 1.4\n",
      "it 1953 - loss 0.3\n",
      "it 1954 - loss 0.8\n",
      "it 1955 - loss 0.4\n",
      "it 1956 - loss 0.7\n",
      "it 1957 - loss 0.5\n",
      "it 1958 - loss 1.2\n",
      "it 1959 - loss 0.9\n",
      "it 1960 - loss 1.2\n",
      "it 1961 - loss 0.9\n",
      "it 1962 - loss 1.1\n",
      "it 1963 - loss 0.7\n",
      "it 1964 - loss 0.2\n",
      "it 1965 - loss 0.8\n",
      "it 1966 - loss 1.2\n",
      "it 1967 - loss 0.8\n",
      "it 1968 - loss 0.5\n",
      "it 1969 - loss 0.9\n",
      "it 1970 - loss 1.0\n",
      "it 1971 - loss 0.4\n",
      "it 1972 - loss 0.8\n",
      "it 1973 - loss 0.9\n",
      "it 1974 - loss 0.4\n",
      "it 1975 - loss 1.3\n",
      "it 1976 - loss 0.5\n",
      "it 1977 - loss 0.2\n",
      "it 1978 - loss 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 1979 - loss 0.7\n",
      "it 1980 - loss 0.8\n",
      "it 1981 - loss 1.4\n",
      "it 1982 - loss 0.2\n",
      "it 1983 - loss 0.4\n",
      "it 1984 - loss 1.7\n",
      "it 1985 - loss 0.4\n",
      "it 1986 - loss 1.1\n",
      "it 1987 - loss 1.0\n",
      "it 1988 - loss 1.0\n",
      "it 1989 - loss 1.4\n",
      "it 1990 - loss 0.4\n",
      "it 1991 - loss 1.4\n",
      "it 1992 - loss 0.8\n",
      "it 1993 - loss 1.5\n",
      "it 1994 - loss 1.2\n",
      "it 1995 - loss 1.2\n",
      "it 1996 - loss 0.3\n",
      "it 1997 - loss 0.9\n",
      "it 1998 - loss 0.5\n",
      "it 1999 - loss 1.4\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(10, 65)  # initialization of the coefficients\n",
    "eta = 0.005  # learning rate (< 1)\n",
    "batch_size = 128\n",
    "loss_history = []\n",
    "it = 0\n",
    "while it < 2000:\n",
    "    # prepare batch\n",
    "    idxs = np.random.choice(range(X_train.shape[0]), size=batch_size, replace=True)\n",
    "    X_batch = X_train[idxs, :]\n",
    "    y_batch = y_train[idxs]\n",
    "    # evaluate loss and gradient\n",
    "    loss, dL_dw = svm_loss_gradient(W, X_batch, y_batch)\n",
    "    print('it {:d} - loss {:.1f}'.format(it, loss))\n",
    "    # gradient descent\n",
    "    W = W - eta * dL_dw\n",
    "    W1.append(W[0, 11])\n",
    "    W2.append(W[5, 35])\n",
    "    loss_history.append(loss)\n",
    "    it += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc9e4f592e8>]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 - 6\n",
      "9 - 9\n",
      "3 - 3\n",
      "7 - 7\n",
      "3 - 2\n",
      "1 - 1\n",
      "5 - 5\n",
      "2 - 2\n",
      "5 - 5\n",
      "2 - 2\n",
      "1 - 1\n",
      "9 - 9\n",
      "4 - 4\n",
      "0 - 0\n",
      "4 - 4\n",
      "2 - 2\n",
      "3 - 3\n",
      "7 - 7\n",
      "8 - 8\n",
      "8 - 8\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    y_pred = np.argmax(np.dot(W, X_test[i]))\n",
    "    print('{} - {}'.format(y_pred, y_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the confusion matrix which is usefull to measure the performances of our multinomial classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = np.argmax(np.dot(W, X_train.T), axis=0)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf = confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'confusion matrix')"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(conf)\n",
    "plt.xlabel('predicted class')\n",
    "plt.ylabel('actual class')\n",
    "plt.title('confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better visualize the errors, it is useful to normalize each row by the total number of samples in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = conf.sum(axis=1, keepdims=True)\n",
    "norm_conf = conf / row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'matrix of error rates')"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.fill_diagonal(norm_conf, 0)\n",
    "plt.imshow(norm_conf)\n",
    "plt.xlabel('predicted class')\n",
    "plt.ylabel('actual class')\n",
    "plt.title('matrix of error rates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns for classes 8 and 9 look worse than the other. Analyzing the type of errors of the model can help improving it.\n",
    "\n",
    "Finally we can plot the optimized weights in (8x8) image form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(251); plt.imshow(W[0, 1:].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(252); plt.imshow(W[1, 1:].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(253); plt.imshow(W[2, 1:].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(254); plt.imshow(W[3, 1:].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(255); plt.imshow(W[4, 1:].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(256); plt.imshow(W[5, 1:].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(257); plt.imshow(W[6, 1:].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(258); plt.imshow(W[7, 1:].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(259); plt.imshow(W[8, 1:].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(2, 5, 10); plt.imshow(W[9, 1:].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare our gradient descent results with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = train_test_split(mnist['data'], mnist['target'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
       "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "              power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - 1\n",
      "3 - 3\n",
      "0 - 0\n",
      "5 - 5\n",
      "5 - 5\n",
      "9 - 9\n",
      "1 - 1\n",
      "8 - 8\n",
      "7 - 7\n",
      "0 - 0\n",
      "3 - 3\n",
      "4 - 4\n",
      "2 - 2\n",
      "9 - 9\n",
      "4 - 4\n",
      "5 - 5\n",
      "2 - 2\n",
      "0 - 0\n",
      "7 - 7\n",
      "5 - 5\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test[:20])\n",
    "for i in range(20):\n",
    "    print('{} - {}'.format(y_pred[i], y_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9867780097425192"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred = clf.predict(X_train)\n",
    "np.sum(y_train_pred == y_train) / X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95867769, 0.95407098, 0.94092827])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(clf, X_train, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y_train_pred = cross_val_predict(clf, X_train, y_train, cv=3)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf = confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'confusion matrix')"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(conf)\n",
    "plt.xlabel('predicted class')\n",
    "plt.ylabel('actual class')\n",
    "plt.title('confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(251); plt.imshow(clf.coef_[0].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(252); plt.imshow(clf.coef_[1].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(253); plt.imshow(clf.coef_[2].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(254); plt.imshow(clf.coef_[3].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(255); plt.imshow(clf.coef_[4].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(256); plt.imshow(clf.coef_[5].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(257); plt.imshow(clf.coef_[6].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(258); plt.imshow(clf.coef_[7].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(259); plt.imshow(clf.coef_[8].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(2, 5, 10); plt.imshow(clf.coef_[9].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
