{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "In this tutorial, we will discover and implement various regression and classification algorithms. We will learn how to utilize of the popular `sklearn` library to implement most of our algorithms. You should see how easy it is to use this library as it consist basically in instantiating a class and calling its method `fit`. For instance to perform a linear regression:\n",
    "\n",
    "```\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, start by importing `numpy` and the `pyplot` module from `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "start by creating some linear data with only one feature using $y=3+4x$ and some gaussian noise. Use $n=100$ to create the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "n = ...\n",
    "x = 3 * np.random.rand(n, 1)\n",
    "y = 3 + 4 * x + np.random.randn(n, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(..., ..., 'o')\n",
    "plt.xlabel(r'$x_1$', fontsize=18)\n",
    "plt.ylabel(r'$y$', rotation=0, fontsize=18)\n",
    "plt.axis([0, 3, 0, 18])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the bias trick to mak a (n, 2) array wit a column of 1 and a column with the first feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones(x.shape[0]), x]\n",
    "print('shape of X with bias: {}'.format(X.shape))\n",
    "print(np.dot(X.T, X).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the optimal solution using the Normal equation: $w=(X^T.X)^{-1}.X^T.y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.linalg.inv(...).dot(...)\n",
    "print(w.shape)\n",
    "print('found optimal parameters w0 = {:.2f} and w1 = {:.2f}'.format(w[0, 0], w[1, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a prediction using our solution and superimpose it to the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.array([0, 3])\n",
    "x_plot = np.c_[np.ones(x_plot.shape[0]), x_plot]\n",
    "y_pred = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'o', label='data points')\n",
    "plt.plot([0, 3], y_pred, '-', lw=3, label='linear fit')\n",
    "plt.xlabel(r'$x_1$', fontsize=18)\n",
    "plt.ylabel(r'$y$', fontsize=18)\n",
    "plt.axis([0, 3, 0, 18])\n",
    "plt.legend()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression works seamlessly in any dimension, let's try an example in 3D. This is useful to find the equation of a plane the best fit a set of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example in 3D\n",
    "np.random.seed(12)\n",
    "x1 = 2 * np.random.rand(100, 1)\n",
    "x2 = 3 * np.random.rand(100, 1)\n",
    "yy = 1 + 2 * x1 + np.random.randn(100, 1) + 3 * x2 + np.random.randn(100, 1)\n",
    "\n",
    "X = np.c_[np.ones(x.shape[0]), x1, x2]\n",
    "w = ...\n",
    "print(w.shape)\n",
    "print('found optimal parameters w0 = {:.2f}, w1 = {:.2f} and w2 = {:.2f}'.format(*w[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#%matplotlib notebook\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# plot the plane surface\n",
    "xx1, xx2 = np.meshgrid(range(3), range(4))\n",
    "yyy = w[0, 0] + w[1, 0] * xx1 + w[2, 0] * xx2\n",
    "ax.plot_surface(..., alpha=0.2)\n",
    "\n",
    "# plot our data points\n",
    "ax.scatter(...)\n",
    "\n",
    "ax.set_xlabel(r'$x_1$')\n",
    "ax.set_ylabel(r'$x_2$')\n",
    "ax.set_zlabel(r'$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start by running a full batch gradient descent and plot the evolution of the prediction. Play with the learning rate $\\eta$ to see the effect of this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "w = np.random.randn(2, 1)\n",
    "print(w)\n",
    "X = np.c_[np.ones(x.shape[0]), x]\n",
    "eta = 0.05\n",
    "n_epochs = 10\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y, 'o', label='data points')\n",
    "y_pred = np.dot(x_plot, w)\n",
    "plt.plot([0, 3], y_pred, 'k--', lw=3, label='initial prediction')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    grad = ...\n",
    "    w = ...\n",
    "    y_pred = ...\n",
    "    plt.plot([0, 3], y_pred, '-', color='orange', lw=2)\n",
    "y_pred = np.dot(x_plot, w)\n",
    "plt.plot([0, 3], y_pred, '-', color='orangered', lw=3, label='final prediction')\n",
    "plt.xlabel(r'$x_1$', fontsize=18)\n",
    "plt.ylabel(r'$y$', fontsize=18)\n",
    "plt.axis([0, 3, -2, 18])\n",
    "plt.legend()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.title('batch GD, {} epochs'.format(n_epochs))\n",
    "plt.savefig('linear_regression_batch_GD.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more time with stochastic gradient descent. Observe that with only 1 epoch the results are very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "w = np.random.randn(2, 1)\n",
    "print(w)\n",
    "eta = 0.05\n",
    "n_epochs = 1\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y, 'o', label='data points')\n",
    "y_pred = np.dot(x_plot, w)\n",
    "plt.plot([0, 3], y_pred, 'k--', lw=3, label='initial prediction')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(n):\n",
    "        index = np.random.randint(n)\n",
    "        Xi = np.atleast_2d(X[index])\n",
    "        grad = ...\n",
    "        w = ...\n",
    "        y_pred = ...\n",
    "        plt.plot([0, 3], y_pred, '-', color='orange', lw=2)\n",
    "y_pred = np.dot(x_plot, w)\n",
    "plt.plot([0, 3], y_pred, '-', color='orangered', lw=3, label='final prediction')\n",
    "plt.xlabel(r'$x_1$', fontsize=18)\n",
    "plt.ylabel(r'$y$', fontsize=18)\n",
    "plt.axis([0, 3, -2, 18])\n",
    "plt.legend()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.title('stochastic GD, {} epochs'.format(n_epochs))\n",
    "plt.savefig('linear_regression_stochastic_GD.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression\n",
    "\n",
    "Polynomial regression is actually equivalent to linear regression except that we do not perform the regression in the same space. Power need to be added to each feature:\n",
    "\n",
    "$$y^i=w_0+w_1 x^i_1+w_2 (x^i_2)^2+\\ldots w_d (x^i_d)^d$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 100\n",
    "X = 6 * np.random.rand(n, 1) - 3\n",
    "y = -0.2*X**3 + 0.5 * X**2 + X + 2 + np.random.randn(n, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(..., ..., 'o')\n",
    "plt.xlabel(\"$x$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the `PolynomialFeatures` class of `sklearn` to automatically add the polynomial features to our array X. With `degree=3`, a column vector is therefore tranformed into a 3 columns array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0])\n",
    "print(X_poly[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform linear regression on the X_poly array. We make use of the `LinearRegression` class of `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(...)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "y_new = lin_reg.predict(X_new_poly)\n",
    "plt.plot(X, y, 'o')\n",
    "plt.plot(X_new, y_new, '-', linewidth=3, label='model')\n",
    "plt.xlabel(r'$x_1$', fontsize=18)\n",
    "plt.ylabel(r'$y$', rotation=0, fontsize=18)\n",
    "plt.legend(loc='upper right', fontsize=14)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.savefig('polynomial_regression.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's play with the degree of polynomial regression $d$. Observe how too low $d$ and too high $d$ lead respectively to underfitting and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "plt.plot(X, y, 'o')\n",
    "\n",
    "d_values = ...\n",
    "lin_reg = LinearRegression()\n",
    "for d in d_values:\n",
    "    poly_features = PolynomialFeatures(degree=d, include_bias=False)\n",
    "    X_poly = poly_features.fit_transform(X)\n",
    "    lin_reg.fit(X_poly, y)\n",
    "    X_new_poly = poly_features.transform(X_new)\n",
    "    y_new = lin_reg.predict(X_new_poly)\n",
    "    plt.plot(X_new, y_new, '-', linewidth=3, label='order %d' % d)\n",
    "\n",
    "plt.xlabel(r'$x_1$', fontsize=18)\n",
    "plt.ylabel(r'$y$', rotation=0, fontsize=18)\n",
    "plt.legend(loc='upper center', fontsize=14)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.savefig('polynomial_regression_varying_degree.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create some data for our regression\n",
    "np.random.seed(42)\n",
    "n = 27\n",
    "X = 6 * np.random.rand(n, 1) - 3\n",
    "X = np.linspace(-3, 3, n, endpoint=True)\n",
    "X = X.reshape((n, 1))\n",
    "y = -0.2*X**3 + 0.5 * X**2 + X + 2 + 1.0 * np.random.randn(n, 1)\n",
    "X_new = np.linspace(-3, 3, 101, endpoint=True).reshape(101, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the closed form solution\n",
    "from sklearn.linear_model import Ridge\n",
    "plt.plot(X, y, 'o')\n",
    "\n",
    "ridge_reg = Ridge(alpha=0.5, solver=\"cholesky\", random_state=42)\n",
    "for d in [3, 1, 30]:\n",
    "    model = Pipeline([\n",
    "            (\"poly_features\", PolynomialFeatures(degree=d, include_bias=False)),\n",
    "            (\"std_scaler\", StandardScaler()),\n",
    "            (\"regul_reg\", ridge_reg),\n",
    "        ])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X_new)\n",
    "    plt.plot(X_new, y_pred, '-', linewidth=3, label='order %d' % d)\n",
    "\n",
    "plt.xlabel(r'$x_1$', fontsize=18)\n",
    "plt.ylabel(r'$y$', rotation=0, fontsize=18)\n",
    "plt.legend(loc='upper center', fontsize=14)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Here we have a look at logistic regression, which is in fact a binary classification model. It is illustrated using the popular iris data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by plotting the logistic function: $\\sigma(x) = \\dfrac{1}{1 + e^{-x}}$, observe that it is continuously derivable and bounded between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 100)\n",
    "sigmoid = ...\n",
    "\n",
    "plt.figure()\n",
    "plt.axhline(y=0., color='gray')\n",
    "plt.axhline(y=0.5, color='gray')\n",
    "plt.axhline(y=1., color='gray')\n",
    "plt.axvline(x=0., color='gray')\n",
    "plt.plot(x, sigmoid, '-', linewidth=3)\n",
    "plt.axis([-10, 10, -0.1, 1.1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assign the petal length and petal width to X and the class target to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[...][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[...] == 2).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(solver='liblinear', C=10**10, random_state=42)\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by creating a grid using the function `meshgrid` of numpy to plot our trained logistic function on the whole domain. Then we will plot also the decision boundary and the data points superimposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(2.9, 7, 500).reshape(-1, 1),\n",
    "        np.linspace(0.8, 2.7, 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "y_proba = ...  # make a prediction here\n",
    "zz = y_proba[:, 1].reshape(x0.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=60, edgecolors='gray')\n",
    "# plot contours and decision boundary\n",
    "left_right = np.array([2.9, 7])\n",
    "boundary = -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1]\n",
    "\n",
    "contour = plt.contour(x0, x1, zz)\n",
    "plt.clabel(contour, inline=1, fontsize=12)\n",
    "plt.plot(left_right, boundary, 'k--', linewidth=3)\n",
    "\n",
    "plt.text(3.5, 1.5, 'Not Iris-Virginica', fontsize=14, ha='center')\n",
    "plt.text(6.5, 2.3, 'Iris-Virginica', fontsize=14, ha='center')\n",
    "plt.xlabel('Petal length', fontsize=14)\n",
    "plt.ylabel('Petal width', fontsize=14)\n",
    "plt.axis([2.9, 7, 0.8, 2.7])\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.savefig('logistic_regression_iris.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax regression\n",
    "\n",
    "To extend the logistic regression to multiple class (multinomial logistic regression), we now study Softmax regression. Here $k$ represent a class index with $K$ classes, $0\\leq k < K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris['data'][:, (2, 3)]  # petal length, petal width\n",
    "y = iris['target']\n",
    "\n",
    "softmax_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10, random_state=42)\n",
    "softmax_reg.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(0, 8, 500).reshape(-1, 1),\n",
    "        np.linspace(0, 3.5, 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "\n",
    "y_proba = softmax_reg.predict_proba(X_new)\n",
    "y_predict = softmax_reg.predict(X_new)\n",
    "zz1 = y_proba[:, 1].reshape(x0.shape)\n",
    "zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "custom_cmap = ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])\n",
    "labels = ['Iris-Virginica', 'Iris-Versicolor', 'Iris-Setosa']\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.contourf(x0, x1, zz, alpha=0.5, cmap=custom_cmap)\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=60, edgecolors='gray', label=labels)\n",
    "contour = plt.contour(x0, x1, zz1, cmap='gray')\n",
    "plt.clabel(contour, inline=1, fontsize=12)\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=labels, title='Class')\n",
    "\n",
    "plt.xlabel('Petal length', fontsize=14)\n",
    "plt.ylabel('Petal width', fontsize=14)\n",
    "#plt.legend(loc='center left', fontsize=14)\n",
    "plt.axis([0, 7, 0, 3.5])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
