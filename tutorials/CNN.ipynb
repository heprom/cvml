{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxXld-8n7avQ"
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "In this tutorial we continue our journey in deep learning by looking at convolutional neural networks or ConvNets. We start by re viewing how convolution work. Then we try to solve a steel microstructure classification problem by using a CNN. We explore how data augmentation, regularization and other gradient descent algorithms may help to better train the model.\n",
    "\n",
    "Run the notebook in Google colab:\n",
    "https://colab.research.google.com/github/heprom/cvml/blob/main/corrections/CNN_cor.ipynb\n",
    "\n",
    "You should use GPU acceleration to train your network, on google colab, before starting working, go to Execution -> Modifier le Type d'Execution -> select GPU as hardware accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8jISbAG7avS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt, cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FZ1MYM27avT"
   },
   "source": [
    "## Understanding convolutions\n",
    "\n",
    "In thisfirst section, we experiment convolutions on images using simple `numpy` operations. We first work with a single channel image from MNIST and then a 3 channel RGB image of a cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cbwtKZ37avT"
   },
   "outputs": [],
   "source": [
    " sklearn.datasets import load_digits\n",
    "mnist = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmBe1SSm7avU"
   },
   "source": [
    "create a variable `image` to hold the first $8\\times8$ image in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoVgZjcP7avU",
    "outputId": "0311c7a6-01a5-4394-c938-01d942be4bc4"
   },
   "outputs": [],
   "source": [
    "image = ...\n",
    "print(image)\n",
    "plt.imshow(..., cmap=cm.gray_r)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUZragZz7avV"
   },
   "source": [
    "create the following $3\\times 3$ kernel as a numpy array: $\\left[\\begin{array}{ccc}-1 & 0 & +1 \\\\ -2 & 0 & +2 \\\\ -1 & 0 & +1\\end{array}\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NV06QuEJ7avV",
    "outputId": "afcd00bf-ea28-4390-e5e0-d81c8586899b"
   },
   "outputs": [],
   "source": [
    "kernel = np.array(...)\n",
    "print(kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QEoYBwo7avW"
   },
   "source": [
    "pad the image with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InPaWd6z7avW",
    "outputId": "9ca3a473-2dc6-4206-f465-5f8bcf0f6cb8"
   },
   "outputs": [],
   "source": [
    "kernel_size = ...\n",
    "pad = ...\n",
    "im = np.pad(image, ((pad, pad), (pad, pad)), mode='constant')\n",
    "print('paded image size is now {}'.format(im.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAPbun6C7avX"
   },
   "source": [
    "convolve the kernel with the image. Create an algorithm using for loops to output the convolution to a new variable `conv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfspDvwH7avX",
    "outputId": "b11c7a30-7629-46c2-b40e-1bc3f5a5bae8"
   },
   "outputs": [],
   "source": [
    "conv = np.empty_like(image)\n",
    "\n",
    "for i in range(pad, im.shape[0] - pad):\n",
    "    for j in range(pad, im.shape[1] - pad):\n",
    "        subset = im[i-pad:i+pad+1, j-pad:j+pad+1]\n",
    "        conv[i-pad, j-pad] = ...  # element-wise multiplication\n",
    "\n",
    "print('output size of the convolution is {}'.format(conv.shape))\n",
    "plt.imshow(conv, cmap=cm.gray)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ox0CPSU17avY"
   },
   "source": [
    "now this works, make a function called `convolve` which takes for input an image, a kernel and output the result of the convolution. Assume image is in form (n x m x channels) and represented by floats in the [0, 1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mVRN90gi7avY"
   },
   "outputs": [],
   "source": [
    "def convolve(image, kernel):\n",
    "    kernel_size = ...\n",
    "    pad = ...\n",
    "    im = np.pad(image, ((pad, pad), (pad, pad)), mode='constant')\n",
    "    conv = np.empty_like(image)\n",
    "\n",
    "    for i in range(pad, im.shape[0] - pad):\n",
    "        for j in range(pad, im.shape[1] - pad):\n",
    "            # get the (i, j) subset of size (2 x pad + 1)\n",
    "            subset = im[i - pad:i + pad + 1, j - pad:j + pad + 1]\n",
    "            # perform the convolution\n",
    "            conv[i - pad, j - pad] = ...\n",
    "    conv = (conv - conv.min()) / (conv.max() - conv.min())  # normalization\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GsREKiBV7avY",
    "outputId": "57611a58-7284-48ec-8f0a-68c5f196452e"
   },
   "outputs": [],
   "source": [
    "plt.imshow(convolve(image, kernel), cmap=cm.gray)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwi60WTL7avZ"
   },
   "source": [
    "Now let's work with a 3 channel RGB image. Load it, convert if to float representation and in gray scale mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5fK5NRA7avZ",
    "outputId": "7f638919-f3e2-403d-ed4d-c503a0c0e488"
   },
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "cat = data.chelsea().astype(np.float)\n",
    "cat = ...  # convert to gray scale\n",
    "cat /= cat.max()  # with float representation, the range is [0, 1]\n",
    "print(cat.shape)\n",
    "print(cat.dtype)\n",
    "print(cat.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ladd2Sz7avZ",
    "outputId": "4bcc0172-fad2-4ee6-d339-ff4834aa4694"
   },
   "outputs": [],
   "source": [
    "plt.imshow(cat, cmap=cm.gray)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bp2JGa9a7avZ"
   },
   "source": [
    "create all the following kernel and try them out:\n",
    "\n",
    " - Blur kernel: $\\left[\\begin{array}{ccc}1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1\\end{array}\\right]$\n",
    " - Laplacian kernel: $\\left[\\begin{array}{ccc}0 & 1 & 0 \\\\ 1 & -4 & 1 \\\\ 0 & 1 & 0\\end{array}\\right]$\n",
    " - Emboss kernel: $\\left[\\begin{array}{ccc}-2 & -1 & 0 \\\\ -1 & 1 & 1 \\\\ 0 & 1 & 2\\end{array}\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGN1tszA7ava"
   },
   "outputs": [],
   "source": [
    "# blur filters\n",
    "blur3 = ...\n",
    "blur5 = ...\n",
    "blur7 = ...\n",
    "\n",
    "# sharpen\n",
    "sharpen = ...\n",
    "\n",
    "# Laplacian kernel\n",
    "laplacian = ...\n",
    "\n",
    "# construct the Sobel x-axis kernel\n",
    "sobelX = np.array(([-1, 0, 1], [-2, 0, 2], [-1, 0, 1]))\n",
    "\n",
    "# construct the Sobel y-axis kernel\n",
    "sobelY = np.array(([-1, -2, -1], [0, 0, 0], [1, 2, 1]))\n",
    "\n",
    "# construct an emboss kernel\n",
    "emboss = ...\n",
    "\n",
    "kernels = [blur3, sharpen, laplacian, emboss]\n",
    "kernel_labels = ['blur3', 'sharpen', 'laplacian', 'emboss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6e8U5BA7ava",
    "outputId": "5d0b5f86-51ef-4ffa-da23-df8be8cd611e"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 7))\n",
    "for i in range(len(kernels)):\n",
    "    ax = plt.subplot(2, 2, i + 1)\n",
    "    convolution = ...\n",
    "    plt.imshow(convolution, cmap=cm.gray, vmin=0.6, vmax=0.9)\n",
    "    plt.title(kernel_labels[i])\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjLfLGvD7avd"
   },
   "source": [
    "## Automated classification of material microstructure : our First ConvNet\n",
    "\n",
    "This model adopts a very classical architecture, so it is perfect to get started with CNN. It can be summarized as:\n",
    "\n",
    "```INPUT => 3 x (CONV => RELU => POOLING) => FC(128) => FC(3)```\n",
    "\n",
    "### As usual start by loading our data set\n",
    "\n",
    "Here we will work with microstructure images with 3 classes: A, B and C. A data set micrographs_64.npz has been created with all the microstructure images and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOv108eF7avd",
    "outputId": "2df7fb33-b0cc-4562-8849-46c63761309d"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "with np.load(...) as data:\n",
    "  X = data['X']\n",
    "  y = data['y']\n",
    "  labels = data['labels']\n",
    "\n",
    "# observe that each image is 3D with one channel (gray level images)\n",
    "print('data set contains %d images of size %d x %d x %d' % (X.shape[0], X.shape[1], X.shape[2], X.shape[3]))\n",
    "\n",
    "# partition the data into training and testing, use 20% for the test set\n",
    "(X_train, X_test, y_train, y_test) = ...\n",
    "\n",
    "# we use the new tensorflow Dataset functionalities \n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 32\n",
    "\n",
    "train_ds = train_ds.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some of the images with the labels to have an idea of how the data looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "nyJWV3Fh7avd",
    "outputId": "daece6b5-4b4e-419e-c7a7-7df67f8c0fe3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "for images, image_labels in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy())\n",
    "    plt.title(labels[image_labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5wH5lvc7ave"
   },
   "source": [
    "Partition the data into training and testing splits using 75% of the data for training and the remaining 25% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "id": "FIEK81Gb7ave",
    "outputId": "ee70a982-ee06-4f16-cae2-f0bb4a3b02ec"
   },
   "outputs": [],
   "source": [
    "# display micrographs from a given class\n",
    "class_A = X_train[np.where(y_train == 0)]\n",
    "class_B = X_train[np.where(y_train == 1)]\n",
    "class_C = X_train[np.where(y_train == 2)]\n",
    "class_A.shape\n",
    "\n",
    "N = 8\n",
    "M = 4\n",
    "indices = np.random.randint(0, class_B.shape[0], size=N*M)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i in range(N * M):\n",
    "    ax = plt.subplot(M, N, i + 1)\n",
    "    plt.imshow(class_B[indices[i]])\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "BDN8z0Y97ave",
    "outputId": "7fd78818-bbb8-4813-ad45-9de3b8875f72"
   },
   "outputs": [],
   "source": [
    "# check the histogram for each class\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "ax1.hist(..., bins=25)\n",
    "plt.title('class A')\n",
    "ax2 = plt.subplot(1, 3, 2)\n",
    "ax2.hist(..., bins=25)\n",
    "plt.title('class B')\n",
    "ax3 = plt.subplot(1, 3, 3)\n",
    "ax3.hist(..., bins=25)\n",
    "plt.title('class C')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efqhvgdj7ave"
   },
   "source": [
    "### Build the model with Keras\n",
    "\n",
    "start importing useful stuff from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmWSEqXb7avf"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "num_classes = len(labels)\n",
    "\n",
    "model = Sequential([\n",
    "  layers.Conv2D(..., 3, padding='same', input_shape=..., activation=...),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(..., 3, padding='same', activation=...),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(..., 3, padding='same', activation=...),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(..., activation='relu'),\n",
    "  layers.Dense(...)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8rC5CS-7-QdE",
    "outputId": "e653edb4-00b3-4ead-95ed-aad098aa15de"
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "epochs = 15\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=test_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d6zPtXz7avf"
   },
   "source": [
    "Save our model to the disk (to reuse it later), this is called **serialization**. In Keras, the architecture of the model and the trained weights are save to a HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5L5scO8_7avf",
    "outputId": "025ec055-75e0-4732-87c4-8d993424d3e1"
   },
   "outputs": [],
   "source": [
    "model.save('microstructure_cnn.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAs4TSQF7avf"
   },
   "source": [
    "Now evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VzsNO7fk7avf",
    "outputId": "1f4d3baa-2330-46eb-a467-709d34f2af62"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(..., batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "P7CMR9MD7avf",
    "outputId": "9d1a61ab-ffdd-431e-8afe-1d1d45af4da8"
   },
   "outputs": [],
   "source": [
    "# on train data\n",
    "index = 5\n",
    "plt.imshow(X_train[index])\n",
    "plt.title('predicted as %c / gt is %c' % (labels[predictions[index].argmax()], labels[y_train[index].argmax()]))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72GFg4Jj_u0E",
    "outputId": "ded985ef-24f6-4452-b6fb-f37e96fbda14"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "cBq5XPpA7avg",
    "outputId": "569c92aa-b2c5-4060-cb46-e771745a5b02"
   },
   "outputs": [],
   "source": [
    "# on test data\n",
    "index = 23\n",
    "plt.imshow(X_test[index])\n",
    "plt.title('predicted as %c / gt is %c' % (labels[predictions[index].argmax()], labels[y_test[index].argmax()]))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h6w_N_CY7avg",
    "outputId": "a61612e0-8b88-4f9e-88b6-84ec20e0aa7d"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions.argmax(axis=1), target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "UpRPN2OM7avg",
    "outputId": "21096d81-666b-49bf-d637-d8b76e6decd4"
   },
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title('Training Loss and Accuracy')\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Loss/Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUmaJvTg7avg"
   },
   "source": [
    "## Using data augmentation and regularization\n",
    "\n",
    "Finally for this tutorial we try this to use data augmentation since we may not have so many images and we start to see some overfitting in our learning phase.\n",
    "\n",
    "Here we will use Keras tools to modify our traning images by flip/rotation/zoom to create new instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "en6PBxIUBJex"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.RandomFlip(\"horizontal\", input_shape=(64, 64, 1)),\n",
    "    layers.RandomRotation(0.25),\n",
    "    layers.RandomZoom(0.2),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "id": "dREraycxBMSy",
    "outputId": "51f0f4f9-9332-47fe-bf99-5d58aad98fee"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "for images, _ in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    augmented_images = data_augmentation(images)\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(augmented_images[0].numpy())\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Thb8-Z2S7avg"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  data_augmentation,\n",
    "  layers.Conv2D(16, 3, padding='same', input_shape=(64, 64, 1), activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.25),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.25),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  #layers.Dropout(0.25),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DONGg6GP7avi"
   },
   "source": [
    "We can also try to compile our model with SGD + Momentum. use a learning rate of 0.01 and learning rate decay (15 epochs). Use the usual value for momentum and activate nesterov acceleration:\n",
    "\n",
    "opt = SGD(learning_rate=0.01, decay=0.01 / 15, momentum=0.9, nesterov=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssLITIsaEqQQ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "opt = SGD(learning_rate=0.01, weight_decay=0.01 / 15, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=opt,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZZJsNtJq7avh",
    "outputId": "6e708b6f-7990-4c84-a29f-c20ec34f4a8d"
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "epochs = 15\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=test_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZal3SCV7avj"
   },
   "source": [
    "Now evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_F9Jl57G7avj",
    "outputId": "3df8e58b-2104-4dbe-e84b-6d4ea91eaf95"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64ooW0HM7avj",
    "outputId": "348d0651-e057-4003-f91d-af6c2a1e628f"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions.argmax(axis=1), target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "MI9-0YHB7avj",
    "outputId": "61093a24-e1b2-4395-a7eb-3de7cfc5d200"
   },
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title('Training Loss and Accuracy')\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Loss/Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piDK4ALp7avk"
   },
   "source": [
    "Finally label a few images with their prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7nkDXTq7avk"
   },
   "outputs": [],
   "source": [
    "N = 8\n",
    "M = 4\n",
    "indices = np.random.randint(0, y_test.shape[0], size=N*M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 665
    },
    "id": "2jSj23zv7avk",
    "outputId": "6a208222-9f7e-409f-faf5-dd9085fbe7cd"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "for i in range(N * M):\n",
    "    ax = plt.subplot(M, N, i + 1)\n",
    "    plt.imshow(X_test[indices[i]])\n",
    "    plt.axis('off')\n",
    "    plt.title('%s' % labels[predictions[indices[i]].argmax()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VN3OCwM87avk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
