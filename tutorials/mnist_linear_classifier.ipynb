{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear model to classify the MNIST data set\n",
    "\n",
    "In this second tutorial, we will continue to work on image classification and try a linear classification model. This kind of model have the same number of parameters as the input images (64 here) plus one bias. They work by trying to with the parameters so that we minimize some Loss function at training time. At test time, a prediction is fast as it is basically just a dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and get a sense of the data\n",
    "\n",
    "We start by loading our image data set: MNIST. Using the function `load_digits` of the `datasets` module of `sklearn` provide the dataset in a reduced form suitable for this practival session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy and initialize the random seed to yield consistent results\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "mnist = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set need to be partitioned into train and test data. Here use the handy function `train_test_split` of `sklearn` to reserve 20% of the data to test your model.\n",
    "\n",
    "**/!\\ The test data is to be left untouched.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(..., test_size=...)\n",
    "\n",
    "print('shape of train data is {}, type is {}'.format(X_train.shape, X_train.dtype))\n",
    "print('shape of test data is {}, type is {}'.format(X_test.shape, X_test.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observe the data points: they are in 64 bits floats but only integers values from 0 to 16. The data can therefore be safely casted to uint8 to reduce the memory footprint by a factor of 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(...)  # min\n",
    "print(...)  # max\n",
    "print(...)  # unique\n",
    "X_train = X_train.astype(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot an image using matplotlib. The function `imshow` can be used reshaping the data as $(8\\times8)$ array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt, cm\n",
    "\n",
    "index = 0\n",
    "plt.imshow(...), cmap=cm.gray_r)\n",
    "plt.axis('off')\n",
    "plt.title('image %d in the train set' % index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this particular dataset, the list of the categories is identical to their indices (from 0 to 9).\n",
    "\n",
    "Print the class of image `index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('image {} is a {}'.format(..., ...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "\n",
    "Here we define our simple machine learning algorithm which takes the features $x$, multiply them be some weights $W$ and add a bias term $b$.\n",
    "\n",
    "$$f(x, W, b) = W.x + b = s$$\n",
    "\n",
    "For a given image in vector form with $d$ features, W has size (10, d) so that the product $W.X$ produces 10 numbers which are called the scores for each class.\n",
    "\n",
    "Initialize `numpy` arrays of size (10, 64) for $W$ and (10) for $b$. Concatenate $b$ and $W$ using the function `np.c_` to use the bias trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization with random weights\n",
    "W = 0.1 * np.random.randn(...)\n",
    "b = 0.1 * np.random.randn(...)\n",
    "\n",
    "# apply the bias trick\n",
    "W = ...\n",
    "print('shape of W is now {}'.format(W.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data points are already in vector form, let's add 1 to each for the bias trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.c_[..., X_train]\n",
    "print('shape of train data is now {}'.format(X_train.shape))\n",
    "\n",
    "X_test = np.c_[..., X_test]\n",
    "print('shape of test data is now {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now compute the 10 scores for the `index` training image with a dot product using `np.dot` and use the max score to determine the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.dot(...)\n",
    "# look at the individual score for each class\n",
    "for (label, score) in zip(labels, scores):\n",
    "    print('{}: {:5.2f}'.format(..., ...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the result, note that as we have 10 scores, we need to find the index of the maximum score to determine the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('prediction: {}'.format(...)\n",
    "print('ground thruth: {}'.format(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "### Hinge loss\n",
    "\n",
    "We now need to define a way to tell the machine how happy we are with this prediction. The machine will then use this information to learn and come up with better predictions. The measure of our \"happiness\" is called a *loss function* and the process of learning the parameters (both $W$ and $b$) is called optimisation.\n",
    "\n",
    "One possibility to measure how good is the prediction is the so called Hinge Loss:\n",
    "\n",
    "$$L_i=\\sum_{j\\neq y^i}\\max(0, s_j - s_{y^i} + 1)$$\n",
    "\n",
    "Since it is inspired by linear support vector machines, this loss is also called Multi-class SVM Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can average arithmetically the losses $L_i$ for each instance $x^i$ to compute the general loss $L$ of the model.\n",
    "\n",
    "$$L=\\frac{1}{n}\\sum_i L_i(f(x^i, W), y^i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step by step calculation of the loss\n",
    "Li = 0\n",
    "yi = ...  # ground truth target\n",
    "for j in range(...):\n",
    "    if j == yi:\n",
    "        print('skipping %d' % j)\n",
    "        continue\n",
    "    margin = ...\n",
    "    print('{:2d} {:6.2f} {:6.2f}'.format(j, scores[j], margin))\n",
    "    Li += ...\n",
    "print(18 * '-')\n",
    "print('hinge loss is {:.1f}'.format(Li))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we understand how the hinge loss works, we can use a more efficient implementation and include it in a reusable function.\n",
    "\n",
    "Create a function (using `def`) called `loss_i` that compute the loss for given parameters `W` and `index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inline calculation of the loss\n",
    "yi = np.squeeze(y_train)[index]\n",
    "Li = np.sum([max(0, scores[j] - scores[yi] + 1) for j in range(10) if j != yi])\n",
    "print(Li)\n",
    "\n",
    "# create a function to evaluate the loss for the given W for image index in the training set\n",
    "def loss_i(...):\n",
    "    yi = ...  # ground truth target\n",
    "    scores = ...\n",
    "    Li = np.sum([max(0, scores[j] - scores[yi] + 1) for j in range(10) if j != yi])\n",
    "    return Li\n",
    "\n",
    "print(loss_i(W, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally create a function to compute the average loss on a batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(W, batch_size=100):\n",
    "    L = 0.  # average loss\n",
    "    for index in range(batch_size):\n",
    "        L += ...\n",
    "    L /= batch_size\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_batch(W, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax loss\n",
    "\n",
    "Another very popular loss function to use with multiclassification problems is the multinomial logistic or softmax loss (popular in deep learning). Here the score for each class is passed to the softmax function: exponentiated (and become positive) and normalized. This gives the probability distribution of this class:\n",
    "\n",
    "$$P(Y=k|X=x_i)=\\frac{e^{s_k}}{\\sum_j e^{s_j}}$$\n",
    "\n",
    "Now we have a probability we can try to maximize the likelihood which is equivalent to minimize the negative of the log likelihood:\n",
    "\n",
    "$$L_i=-\\log P(Y=k|X=x_i)=-\\log\\left(\\frac{e^{s_k}}{\\sum_j e^{s_j}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by exponentiating our scores to obtain unnormalized probabilities\n",
    "escores = np.exp(scores)\n",
    "norm_escores = escores / np.sum(escores)\n",
    "for j in range(10):\n",
    "    print('{:6d} | {:8.1f} | {:6.4f}'.format(j, escores[j], norm_escores[j]))\n",
    "print(26 * '-')\n",
    "# verify that the sum of the probability is 1\n",
    "print('sum of probabilities check: {:.3f}'.format(np.sum(norm_escores)))\n",
    "# compute the softmax loss\n",
    "Li = -np.log(norm_escores[yi])\n",
    "print('Softmax loss is {:.2f}'.format(Li))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the model\n",
    "\n",
    "Here we use the calculated loss to optimize the parameters $W$ and $b$. For this we need to evaluate the gradient $\\dfrac{\\partial L}{\\partial W}$ of $L$ with respect to $W$.\n",
    "\n",
    "The gradient is obtained by differentiating the loss expression with respect to $W$:\n",
    "\n",
    "$$\\nabla_{w_j}L_i=1\\left(w_j^T x_i - w_{y_i}^T x_i + 1 > 0\\right) x_i\\quad\\text{for }j\\neq y_i$$\n",
    "\n",
    "$$\\nabla_{w_{y_i}}L_i=-\\left(\\sum_{j\\neq y_i}1\\left(w_j^T x_i - w_{y_i}^T x_i + 1 > 0\\right)\\right) x_i$$\n",
    "\n",
    "with $1(condition)$ equals to 1 if $condition$ is true, 0 otherwise. Here we see that the data vector $x$ is scaled by the number of classes that did not meet the margins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify one more time the size of our matrices\n",
    "print('shape of train data is {}'.format(X_train.shape))\n",
    "print('shape of W is {}'.format(W.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Simple SVM loss gradient implementation:\n",
    " - iterate over each data point $i$ in the batch\n",
    " - compute the score using $W.x^i$ (bias trick)\n",
    " - compute the margin for each class\n",
    " - compute the loss and the gradient components associated with this data point\n",
    " - finally average the gradient and the loss with respect to the number of data points in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss_gradient(W, X, y):\n",
    "    \"\"\"\n",
    "    SVM loss gradient.\n",
    "\n",
    "    Inputs:\n",
    "    - W: array of shape (K, 1 + D) containing the weights.\n",
    "    - X: array of shape (N, 1 + D) containing the data.\n",
    "    - y: array of shape (N, 1) containing training labels 0 <= k < K.\n",
    "    Returns a tuple of:\n",
    "    - average loss\n",
    "    - gradient of the loss with respect to weights W\n",
    "    \"\"\"\n",
    "    dW = np.zeros_like(W)  # initialize the gradient as zero\n",
    "    K = ...  # number of classes\n",
    "    n = ...  # number of data points\n",
    "    loss = 0.0\n",
    "    for i in range(n):\n",
    "        #print('evaluating gradient / image %d' % i)\n",
    "        yi = np.squeeze(y)[i]  # ground truth target\n",
    "        scores = ...\n",
    "        # compute SVM loss and gradient for this data point\n",
    "        for j in range(K):\n",
    "            if j == yi:\n",
    "                continue\n",
    "            # only compute loss if incorrectly classified\n",
    "            margin = ...\n",
    "            if margin > 0:\n",
    "                loss += margin\n",
    "                dW[yi, :] -= ...  # correct class gradient\n",
    "                dW[j, :] += ...  # incorrect class gradient\n",
    "\n",
    "    # average the loss and gradient\n",
    "    loss /= n\n",
    "    dW /= n\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try our SVM gradient loss by computing the gradient with respect to the first `nb` images in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = 100\n",
    "loss, dW = svm_loss_gradient(...)\n",
    "print('loss is {:.2f}'.format(loss))\n",
    "print('gradient dW with respect to the first pixel =', dW[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient check\n",
    "\n",
    "now, to verify our SVM gradietn implementation, we are going to perform a **gradient check**. \n",
    "\n",
    "The gradient is computed numerically using a finite difference scheme:\n",
    "\n",
    "$$\\nabla L\\approx\\dfrac{L(W+h) - L(W-h)}{2h}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(f, W, h=0.0001):\n",
    "    dL = np.zeros_like(W)\n",
    "    # evaluate the loss modifiying each value of W\n",
    "    for c in range(W.shape[0]):\n",
    "        for p in range(W.shape[1]):\n",
    "            W[c, p] += h\n",
    "            fxph = f(W)\n",
    "            W[c, p] -= 2*h\n",
    "            fxmh = f(W)\n",
    "            dL[c, p] = ...  # centered finite differences\n",
    "            W[c, p] += h  # put back initial value\n",
    "    return dL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply our gradient check, print the gradient with respect to the first pixel. Compare with the analytical value. Realize that to evaluate the gradient numerically, the loss function was called $2\\times64$ times. This is why it is so slow. And we tested it only with 100 training images over 1437!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loss is {:.2f}'.format(loss_batch(W, batch_size=100)))\n",
    "dL = gradient_check(loss_batch, W)\n",
    "print(dL.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dL[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "now we have successfully created our linear model, loss function, and that we can compute the gradient of the loss with respect to $W$, let's actually use this to perform gradient descent and learn our model.\n",
    "\n",
    "The backbone of the gradient descent is this simple equation:\n",
    "$$W\\leftarrow W - \\eta \\nabla_W L$$\n",
    "\n",
    "$\\eta$ is the learning rate (the most important hyperparameter). The weights $W$ are being updated at each iteration until a stop criterion is met or a maximum number of iteration reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine one single gradient descent step\n",
    "W = 0.1 * np.random.randn(10, 65)\n",
    "print('average loss is %.1f' % loss_batch(W, batch_size=X_train.shape[0]))\n",
    "loss, dL_dw = svm_loss_gradient(W, X_train, y_train)\n",
    "\n",
    "# perform one gradient descent\n",
    "eta = 0.005\n",
    "W = W - eta * dL_dw\n",
    "print('after one step the average loss is %.1f' % loss_batch(W, batch_size=X_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent\n",
    "\n",
    "because $n$ is large (1437 here, but can also be much much larger), it does not actually make sense of computing the gradient on the complete set of training images at each iteration (remeber that the gradient is averaged). Instead, it is very common to compute the gradient on a subset (called a mini-batch) of 32 to 256 images. This is much faster and performs well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(10, 65)  # initialization of the coefficients\n",
    "eta = 0.005  # learning rate (< 1)\n",
    "batch_size = 128\n",
    "loss_history = []\n",
    "it = 0\n",
    "while it < 2000:\n",
    "    # prepare batch\n",
    "    idxs = np.random.choice(range(X_train.shape[0]), size=batch_size, replace=True)\n",
    "    X_batch = X_train[idxs, :]\n",
    "    y_batch = y_train[idxs]\n",
    "    # evaluate loss and gradient\n",
    "    loss, dL_dw = ...\n",
    "    print('it {:d} - loss {:.1f}'.format(it, loss))\n",
    "    # gradient descent\n",
    "    W = ...\n",
    "    loss_history.append(loss)\n",
    "    it += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make some prediction! Try the first 20 entries in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    y_pred = ...\n",
    "    print('{} - {}'.format(y_pred, y_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the confusion matrix which is usefull to measure the performances of our multinomial classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_train_pred = ...\n",
    "conf = confusion_matrix(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(conf)\n",
    "plt.xlabel('predicted class')\n",
    "plt.ylabel('actual class')\n",
    "plt.title('confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better visualize the errors, it is useful to normalize each row by the total number of samples in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = conf.sum(axis=1, keepdims=True)\n",
    "norm_conf = conf / row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(norm_conf, 0)\n",
    "plt.imshow(norm_conf)\n",
    "plt.xlabel('predicted class')\n",
    "plt.ylabel('actual class')\n",
    "plt.title('matrix of error rates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns for classes 8 and 9 look worse than the other. Analyzing the type of errors of the model can help improving it.\n",
    "\n",
    "Finally we can compare our results with the `SGDClassifier` from `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare our gradient descent results with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = train_test_split(mnist['data'], mnist['target'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(...)\n",
    "for i in range(20):\n",
    "    print('{} - {}'.format(y_pred[i], y_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the **accuracy** by dividing the number of correct prediction in the train set by the number os training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = clf.predict(...)\n",
    "print(np.sum(...) / ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is better to perform K-fold cross validation to measure the performances of the model. For this we can use the `cross_val_score` method with `cv=3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(clf, X_train, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y_train_pred = cross_val_predict(clf, X_train, y_train, cv=3)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf = confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(conf)\n",
    "plt.xlabel('predicted class')\n",
    "plt.ylabel('actual class')\n",
    "plt.title('confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(251); plt.imshow(clf.coef_[0].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(252); plt.imshow(clf.coef_[1].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(253); plt.imshow(clf.coef_[2].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(254); plt.imshow(clf.coef_[3].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(255); plt.imshow(clf.coef_[4].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(256); plt.imshow(clf.coef_[5].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(257); plt.imshow(clf.coef_[6].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(258); plt.imshow(clf.coef_[7].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(259); plt.imshow(clf.coef_[8].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.subplot(2, 5, 10); plt.imshow(clf.coef_[9].reshape((8, 8)), cmap=cm.gray); plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
