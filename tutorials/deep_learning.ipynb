{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "In this notebook, we move from single layer to networks with at least 2 layers. With this the numebr of neurons drammattically increases. Building on previous tutorial, we will program our own class to handle any number of layers. We will first train a very simple 2 layer network to solve the XOR problem and verify that we can target non linearly separable data sets. We then will train a larger 2 hidden layer network to solve the MNIST data set with an accuracy of more that 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the notebook in Google colab:\n",
    "https://colab.research.google.com/github/heprom/cvml/blob/main/tutorials/deep_learning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Implementation of a two layer NN\n",
    "\n",
    "Two layers with two inputs: $w^{(1)}$ has size (3, 3) including the bias and $w^{(2)}$ has size (3, 1) to give one output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt, cm\n",
    "\n",
    "X = np.array(...)  # include term for the bias trick\n",
    "y = np.array(...)\n",
    "\n",
    "np.random.seed(2)\n",
    "w1 = np.random.randn(...)  # first layer\n",
    "w2 = np.random.randn(...)  # second layer\n",
    "print('* model params: {}, {}'.format(w1.tolist(), w2.tolist()))\n",
    "eta = 1e-2  # learning rate\n",
    "n_epochs = 10000\n",
    "\n",
    "for t in range(n_epochs):\n",
    "    # forward pass\n",
    "    h = ...  # activation of the first layer\n",
    "    y_pred = ...  # activation of the second layer\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # backprop\n",
    "    grad_y_pred = ...\n",
    "    grad_w2 = ...\n",
    "    grad_h = ...\n",
    "    grad_w1 = ...\n",
    "\n",
    "    # update rule\n",
    "    w1 -= ...\n",
    "    w2 -= ...\n",
    "print('* new model params: {}'.format(w1.tolist(), w2.tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print the results of the predictions with our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (xi, yi) in zip(X, y):\n",
    "    h = 1 / (1 + np.exp(-xi.dot(w1)))\n",
    "    out = ...\n",
    "    y_pred = 1 if out > 0.5 else 0\n",
    "    print('data={}, ground-truth={}, out={:.3f}, y={}'.format(xi, yi, out[0], y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Implementation\n",
    "\n",
    "We modify our previous implementation of a Neural Network to include any number of layers. Additionnally, each layer can have any number of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"Artificial Neural Network class. \n",
    "    \n",
    "    A general Feed-Forward Neural Network. Here, the activation function is a sigmoid, \n",
    "    the loss is computed using the squared error between the target and \n",
    "    the prediction. Learning the parameters is achieved using back-propagation \n",
    "    and gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layers, eta=0.1, rand_seed=42):\n",
    "        \"\"\"Initialisation routine.\"\"\"\n",
    "        np.random.seed(rand_seed)\n",
    "        self.W = []\n",
    "        self.layers = layers  # keep a record of this\n",
    "        # loop on the layers except for the last one\n",
    "        for i in np.arange(...):\n",
    "            w = np.random.randn(...)\n",
    "            self.W.append(w)\n",
    "        # the last layer does not need a bias\n",
    "        w = np.random.randn(...)\n",
    "        self.W.append(w)\n",
    "        self.eta = eta  # learning rate\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"Simple string representation of the network.\"\"\"\n",
    "        return \"NeuralNetwork: {}\".format('-'.join(str(l) for l in self.layers))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Our activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_grad(self, x):\n",
    "        \"\"\"Gradient of the sigmoid function.\"\"\"\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "    \n",
    "    def predict(self, X, bias_trick=True):\n",
    "        \"\"\"Compute the output of the network for the input X. \n",
    "        Notice that out value of bounded between 0 and 1.\"\"\"\n",
    "        p = np.atleast_2d(X)\n",
    "        if bias_trick:\n",
    "            # bias trick\n",
    "            p = np.c_[np.ones((p.shape[0])), p]\n",
    "        for layer in np.arange(0, len(self.layers) - 1):\n",
    "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "        return p\n",
    "    \n",
    "    def loss(self, X, y, bias_trick=False):\n",
    "        \"\"\"Compute the squared error loss for a given set of inputs.\"\"\"\n",
    "        y = np.atleast_2d(y)\n",
    "        y_pred = self.predict(X, bias_trick=bias_trick)\n",
    "        loss = np.sum((y_pred - y) ** 2)\n",
    "        return loss\n",
    "        \n",
    "    def back_propagation(self, X, y):\n",
    "        \"\"\"Conduct backpropagation to update the weights.\"\"\"\n",
    "        # we need to keep a list of the activation of each layer\n",
    "        A = [np.atleast_2d(X)]\n",
    "        \n",
    "        # forward pass\n",
    "        for layer in np.arange(...):\n",
    "            a = ....\n",
    "            A.append(...)\n",
    "        \n",
    "        # backprop phase\n",
    "        D = [(A[-1] - y) * A[-1] * (1 - A[-1])]\n",
    "        for layer in np.arange(...):\n",
    "            delta = ...\n",
    "            delta = ...\n",
    "            D.append(delta)\n",
    "        D = D[::-1]  # reverse the order\n",
    "        \n",
    "        # update weights\n",
    "        for layer in np.arange(0, len(self.layers) - 1):\n",
    "            grad_W = ...\n",
    "            self.W[layer] -= ...\n",
    "        \n",
    "    def fit(self, X, y, n_epochs=10, method='batch', display_update=100):\n",
    "        \"\"\"Perform gradient descent on a given number of epochs to update the weights.\"\"\"\n",
    "        # bias trick: add a column of 1 to X\n",
    "        X = np.c_[np.ones((X.shape[0])), X]\n",
    "        for i_epoch in range(n_epochs):\n",
    "            if method == 'batch':\n",
    "                # perform backprop on the whole training set (batch)\n",
    "                self.back_propagation(X, y)\n",
    "            else:\n",
    "                # here we update the weight for every data point (SGD)\n",
    "                for (xi, yi) in zip(X, y):\n",
    "                    self.back_propagation(xi, yi)\n",
    "            # an epoch has passed, compute the loss\n",
    "            loss = self.loss(X, y)\n",
    "            self.loss_history.append(loss)\n",
    "            if i_epoch == 0 or (1 + i_epoch) % display_update == 0:\n",
    "                print(\"epoch={}, loss={:.3f}\".format(1 + i_epoch, loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our 2-2-1 neural network and train it\n",
    "np.random.seed(42)\n",
    "nn = NeuralNetwork(layers=[...], eta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test our __repr__function\n",
    "print(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "print('W shape: {}'.format([Wi.shape for Wi in nn.W]))\n",
    "print('initial weights:', nn.W)\n",
    "nn.fit(..., ..., n_epochs=1000, method='SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (xi, yi) in zip(X, y):\n",
    "    out = ...\n",
    "    y_pred = 1 if out > 0.5 else 0\n",
    "    print('data={}, ground-truth={}, out={:.3f}, y={}'.format(xi, yi, out[0][0], y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4.5, 4))\n",
    "plt.plot(..., linewidth=3)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.subplots_adjust(left=0.15, top=0.95)\n",
    "plt.savefig('XOR_deep_learning_loss.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the MNIST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits['images'].shape)\n",
    "print(digits['images'].dtype)\n",
    "print(digits['images'].min())\n",
    "print(digits['images'].max())\n",
    "print(digits['data'].shape)\n",
    "print(digits['data'].dtype)\n",
    "\n",
    "index = 34\n",
    "print(digits['data'][index, :].reshape((8, 8)))\n",
    "np.sum(digits['data'][index, :] == digits['images'][index].ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the image with index `index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(..., cmap=cm.gray_r)\n",
    "plt.title('ground-truth: %d' % digits.target[index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "have a look at the first few images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 20\n",
    "fig, axes = plt.subplots(1, n_images)\n",
    "for i in range(n_images):\n",
    "    ax = axes[i]\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(digits['images'][i], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('%i' % digits['target'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training and a testing set\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(digits['data'] / 16.0, digits['target'], test_size=0.25, random_state=13)\n",
    "print(X_train.shape)\n",
    "print('first 10 train labels: {}'.format(y_train[:10]))\n",
    "print('first 10 test labels: {}'.format(y_test[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify training images\n",
    "fig, axes = plt.subplots(1, n_images)\n",
    "for i in range(n_images):\n",
    "    ax = axes[i]\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(X_train[i].reshape((8, 8)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('%i' % y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training images\n",
    "start_index = 10\n",
    "plt.figure(figsize=(15, 9))\n",
    "for i, index in enumerate(range(start_index, start_index + 180)):\n",
    "    ax = plt.subplot(9, 20, i + 1)\n",
    "    ax.imshow(X_train[index, :].reshape((8, 8)), cmap=cm.gray_r)\n",
    "    plt.axis('off')\n",
    "plt.axis('tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our network implementation, the output of the network is bounded between 0 and 1 (output of the sigmoidal function). We could have implemented a multinomial SVM classifier but instead we will use autoencoders to keep it very general.\n",
    "\n",
    "Each digit is transformed into a 10 component vector of zero axecpt for a 1 at the position of the digit. For example 6 become [0, 0, 0, 0, 0, 1, 0, 0, 0]. This allows to create a 10 output network and to compare each output with the transformed version of each label.\n",
    "\n",
    "We use `scikit-learn` utility `LabelBinarizer` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# convert the labels from integers to vectors\n",
    "y_train = LabelBinarizer().fit_transform(y_train)\n",
    "y_test = LabelBinarizer().fit_transform(y_test)\n",
    "print(y_train.shape)\n",
    "for i in range(10):\n",
    "    print('{}: {}'.format(y_train[i].argmax(), y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create our neural network with 4 layers of 64, 32, 16 and finally 10 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "nn = NeuralNetwork(...)\n",
    "print(nn)\n",
    "print(\"learning rate {}\".format(nn.eta))\n",
    "\n",
    "nn.fit(..., ..., n_epochs=100, method='SGD', display_update=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss saved during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4.5, 4))\n",
    "plt.plot(..., linewidth=3)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.subplots_adjust(left=0.15, top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at a particular data point from the test set\n",
    "index = 11\n",
    "y_pred = nn.predict(X_test[index, :])\n",
    "y_class = ...\n",
    "y = ...\n",
    "print('test image {}: predict a {} - ground truth is a {}'.format(index, y_class, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "start_index = 10\n",
    "plt.figure(figsize=(12, 9))\n",
    "for i, index in enumerate(range(start_index, start_index + 60)):\n",
    "    y_pred = nn.predict(X_test[index, :])\n",
    "    y_class = np.argmax(y_pred)\n",
    "    y = np.argmax(y_test[index])\n",
    "    if y_class == y:\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    ax = plt.subplot(6, 10, i + 1)\n",
    "    ax.imshow(X_test[index, :].reshape((8, 8)), cmap=cm.gray_r)\n",
    "    rect = patches.Rectangle((-0.5, -0.5), 8.0, 8.0, linewidth=8, edgecolor=color, facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    #plt.title('seen as {}'.format(y_class))\n",
    "    plt.axis('off')\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = nn.predict(X_test)\n",
    "y_test_pred = predictions.argmax(axis=1)\n",
    "print(classification_report(y_test.argmax(axis=1), y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "predictions = ...\n",
    "y_train_pred = predictions.argmax(axis=1)\n",
    "conf = confusion_matrix(y_train.argmax(axis=1), y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(...)\n",
    "plt.xlabel('predicted class')\n",
    "plt.ylabel('actual class')\n",
    "plt.title('confusion matrix')\n",
    "plt.savefig('mnist_deep_learning_confusion_matrix.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tansorflow+Keras\n",
    "\n",
    "Keras is a high level programming interface to build deep Neural Networks. It need a backend to run (to actually build te computational graph, compute the gradients and so on).\n",
    "\n",
    "Note that starting from version 2.3.0, Keras is now part of TensorFlow (which is now the default and only backend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Sigmoid layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the 64-32-16-10 architecture using Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(..., input_shape=(64,), activation='sigmoid'))\n",
    "model.add(Dense(..., activation='sigmoid'))\n",
    "model.add(Dense(..., activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(0.01)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=sgd, \n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model on 300 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training network...')\n",
    "H = model.fit(\n",
    "    ..., \n",
    "    ..., \n",
    "    validation_data=(X_test, y_test), \n",
    "    epochs=..., \n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    "    batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot of the training/validation loss and of the training/validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.axhline(y=1., color='gray', linestyle='dashed')\n",
    "plt.plot(H.history[\"loss\"][:300], label=\"training loss\")\n",
    "plt.plot(H.history[\"val_loss\"][:300], label=\"validation loss\")\n",
    "plt.plot(H.history[\"accuracy\"][:300], label=\"training accuracy\")\n",
    "plt.plot(H.history[\"val_accuracy\"][:300], label=\"validation accuracy\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.xlim(0, 300)\n",
    "plt.legend()\n",
    "plt.savefig('Keras_mnist.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ReLU layers\n",
    "\n",
    "In practice, you should always try ReLU over Sigmoid as it as proven to provide a better learning ability to the network (help with vanishing gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_relu = Sequential()\n",
    "model_relu.add(Dense(32, input_shape=(64, ), activation='relu'))\n",
    "model_relu.add(Dense(16, activation='relu'))\n",
    "model_relu.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_relu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(0.01)\n",
    "model_relu.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=sgd, \n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = model_relu.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=300, \n",
    "    validation_split=0.1, \n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    "    batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = H.history['loss']\n",
    "val_loss = H.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "ax1.plot(epochs, loss, label='Training loss')\n",
    "ax1.plot(epochs, val_loss, label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "acc = H.history['accuracy']\n",
    "val_acc = H.history['val_accuracy']\n",
    "ax2.plot(epochs, acc, label='Training acc')\n",
    "ax2.plot(epochs, val_acc, label='Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
