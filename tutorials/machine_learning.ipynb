{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "In this tutorial, we will discover and implement various regression and classification algorithms. We will learn how to utilize of the popular `sklearn` library to implement most of our algorithms. You should see how easy it is to use this library as it consist basically in instantiating a class and calling its method `fit`. For instance to perform a linear regression:\n",
    "\n",
    "```\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the notebook in Google colab:\n",
    "https://colab.research.google.com/github/heprom/cvml/blob/main/tutorials/machine_learning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, start by importing `numpy` and the `pyplot` module from `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "start by creating some linear data with only one feature using $y=3+4x$ and some gaussian noise. Use $n=100$ to create the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "n = ...\n",
    "x = 3 * np.random.rand(n, 1)\n",
    "y = 3 + 4 * x + np.random.randn(n, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(..., ..., 'o')\n",
    "plt.xlabel(r'$x_1$', fontsize=18)\n",
    "plt.ylabel(r'$y$', rotation=0, fontsize=18)\n",
    "plt.axis([0, 3, 0, 18])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the bias trick to mak a (n, 2) array wit a column of 1 and a column with the first feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones(x.shape[0]), x]\n",
    "print('shape of X with bias: {}'.format(X.shape))\n",
    "print(np.dot(X.T, X).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the optimal solution using the Normal equation: $w=(X^T.X)^{-1}.X^T.y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.linalg.inv(...).dot(...)\n",
    "print(w.shape)\n",
    "print('found optimal parameters w0 = {:.2f} and w1 = {:.2f}'.format(w[0, 0], w[1, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a prediction using our solution and superimpose it to the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.array([0, 3])\n",
    "x_plot = np.c_[np.ones(x_plot.shape[0]), x_plot]\n",
    "y_pred = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'o', label='data points')\n",
    "plt.plot([0, 3], y_pred, '-', lw=3, label='linear fit')\n",
    "plt.xlabel(r'$x_1$', fontsize=18)\n",
    "plt.ylabel(r'$y$', fontsize=18)\n",
    "plt.axis([0, 3, 0, 18])\n",
    "plt.legend()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression works seamlessly in any dimension, let's try an example in 3D. This is useful to find the equation of a plane the best fit a set of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example in 3D\n",
    "np.random.seed(12)\n",
    "x1 = 2 * np.random.rand(100, 1)\n",
    "x2 = 3 * np.random.rand(100, 1)\n",
    "yy = 1 + 2 * x1 + np.random.randn(100, 1) + 3 * x2 + np.random.randn(100, 1)\n",
    "\n",
    "X = np.c_[np.ones(x.shape[0]), x1, x2]\n",
    "w = ...\n",
    "print(w.shape)\n",
    "print('found optimal parameters w0 = {:.2f}, w1 = {:.2f} and w2 = {:.2f}'.format(*w[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#%matplotlib notebook\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# plot the plane surface\n",
    "xx1, xx2 = np.meshgrid(range(3), range(4))\n",
    "yyy = w[0, 0] + w[1, 0] * xx1 + w[2, 0] * xx2\n",
    "ax.plot_surface(..., alpha=0.2)\n",
    "\n",
    "# plot our data points\n",
    "ax.scatter(...)\n",
    "\n",
    "ax.set_xlabel(r'$x_1$')\n",
    "ax.set_ylabel(r'$x_2$')\n",
    "ax.set_zlabel(r'$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start by running a full batch gradient descent and plot the evolution of the prediction. Play with the learning rate $\\eta$ to see the effect of this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "w = np.random.randn(2, 1)\n",
    "print(w)\n",
    "X = np.c_[np.ones(x.shape[0]), x]\n",
    "eta = 0.05\n",
    "n_epochs = 10\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y, 'o', label='data points')\n",
    "y_pred = np.dot(x_plot, w)\n",
    "plt.plot([0, 3], y_pred, 'k--', lw=3, label='initial prediction')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    grad = ...\n",
    "    w = ...\n",
    "    y_pred = ...\n",
    "    plt.plot([0, 3], y_pred, '-', color='orange', lw=2)\n",
    "y_pred = np.dot(x_plot, w)\n",
    "plt.plot([0, 3], y_pred, '-', color='orangered', lw=3, label='final prediction')\n",
    "plt.xlabel(r'$x_1$', fontsize=18)\n",
    "plt.ylabel(r'$y$', fontsize=18)\n",
    "plt.axis([0, 3, -2, 18])\n",
    "plt.legend()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.title('batch GD, {} epochs'.format(n_epochs))\n",
    "plt.savefig('linear_regression_batch_GD.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more time with stochastic gradient descent. Observe that with only 1 epoch the results are very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "w = np.random.randn(2, 1)\n",
    "print(w)\n",
    "eta = 0.05\n",
    "n_epochs = 1\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y, 'o', label='data points')\n",
    "y_pred = np.dot(x_plot, w)\n",
    "plt.plot([0, 3], y_pred, 'k--', lw=3, label='initial prediction')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(n):\n",
    "        index = np.random.randint(n)\n",
    "        Xi = np.atleast_2d(X[index])\n",
    "        grad = ...\n",
    "        w = ...\n",
    "        y_pred = ...\n",
    "        plt.plot([0, 3], y_pred, '-', color='orange', lw=2)\n",
    "y_pred = np.dot(x_plot, w)\n",
    "plt.plot([0, 3], y_pred, '-', color='orangered', lw=3, label='final prediction')\n",
    "plt.xlabel(r'$x_1$', fontsize=18)\n",
    "plt.ylabel(r'$y$', fontsize=18)\n",
    "plt.axis([0, 3, -2, 18])\n",
    "plt.legend()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.title('stochastic GD, {} epochs'.format(n_epochs))\n",
    "plt.savefig('linear_regression_stochastic_GD.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression\n",
    "\n",
    "Polynomial regression is actually equivalent to linear regression except that we do not perform the regression in the same space. Power need to be added to each feature:\n",
    "\n",
    "$$y^i=w_0+w_1 x^i_1+w_2 (x^i_2)^2+\\ldots w_d (x^i_d)^d$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 100\n",
    "X = 6 * np.random.rand(n, 1) - 3\n",
    "y = -0.2*X**3 + 0.5 * X**2 + X + 2 + np.random.randn(n, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(..., ..., 'o')\n",
    "plt.xlabel(\"$x$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the `PolynomialFeatures` class of `sklearn` to automatically add the polynomial features to our array X. With `degree=3`, a column vector is therefore tranformed into a 3 columns array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0])\n",
    "print(X_poly[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform linear regression on the X_poly array. We make use of the `LinearRegression` class of `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(...)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "y_new = lin_reg.predict(X_new_poly)\n",
    "plt.plot(X, y, 'o')\n",
    "plt.plot(X_new, y_new, '-', linewidth=3, label='model')\n",
    "plt.xlabel(r'$x_1$', fontsize=18)\n",
    "plt.ylabel(r'$y$', rotation=0, fontsize=18)\n",
    "plt.legend(loc='upper right', fontsize=14)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.savefig('polynomial_regression.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's play with the degree of polynomial regression $d$. Observe how too low $d$ and too high $d$ lead respectively to underfitting and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "plt.plot(X, y, 'o')\n",
    "\n",
    "d_values = ...\n",
    "lin_reg = LinearRegression()\n",
    "for d in d_values:\n",
    "    poly_features = PolynomialFeatures(degree=d, include_bias=False)\n",
    "    X_poly = poly_features.fit_transform(X)\n",
    "    lin_reg.fit(X_poly, y)\n",
    "    X_new_poly = poly_features.transform(X_new)\n",
    "    y_new = lin_reg.predict(X_new_poly)\n",
    "    plt.plot(X_new, y_new, '-', linewidth=3, label='order %d' % d)\n",
    "\n",
    "plt.xlabel(r'$x_1$', fontsize=18)\n",
    "plt.ylabel(r'$y$', rotation=0, fontsize=18)\n",
    "plt.legend(loc='upper center', fontsize=14)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.savefig('polynomial_regression_varying_degree.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create some data for our regression\n",
    "np.random.seed(42)\n",
    "n = 27\n",
    "X = 6 * np.random.rand(n, 1) - 3\n",
    "X = np.linspace(-3, 3, n, endpoint=True)\n",
    "X = X.reshape((n, 1))\n",
    "y = -0.2*X**3 + 0.5 * X**2 + X + 2 + 1.0 * np.random.randn(n, 1)\n",
    "X_new = np.linspace(-3, 3, 101, endpoint=True).reshape(101, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the closed form solution\n",
    "from sklearn.linear_model import Ridge\n",
    "plt.plot(X, y, 'o')\n",
    "\n",
    "ridge_reg = Ridge(alpha=0.5, solver=\"cholesky\", random_state=42)\n",
    "for d in [3, 1, 30]:\n",
    "    model = Pipeline([\n",
    "            (\"poly_features\", PolynomialFeatures(degree=d, include_bias=False)),\n",
    "            (\"std_scaler\", StandardScaler()),\n",
    "            (\"regul_reg\", ridge_reg),\n",
    "        ])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X_new)\n",
    "    plt.plot(X_new, y_pred, '-', linewidth=3, label='order %d' % d)\n",
    "\n",
    "plt.xlabel(r'$x_1$', fontsize=18)\n",
    "plt.ylabel(r'$y$', rotation=0, fontsize=18)\n",
    "plt.legend(loc='upper center', fontsize=14)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Here we have a look at logistic regression, which is in fact a binary classification model. It is illustrated using the popular iris data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by plotting the logistic function: $\\sigma(x) = \\dfrac{1}{1 + e^{-x}}$, observe that it is continuously derivable and bounded between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 100)\n",
    "sigmoid = ...\n",
    "\n",
    "plt.figure()\n",
    "plt.axhline(y=0., color='gray')\n",
    "plt.axhline(y=0.5, color='gray')\n",
    "plt.axhline(y=1., color='gray')\n",
    "plt.axvline(x=0., color='gray')\n",
    "plt.plot(x, sigmoid, '-', linewidth=3)\n",
    "plt.axis([-10, 10, -0.1, 1.1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assign the petal length and petal width to X and the class target to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[...][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[...] == 2).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(solver='liblinear', C=10**10, random_state=42)\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by creating a grid using the function `meshgrid` of numpy to plot our trained logistic function on the whole domain. Then we will plot also the decision boundary and the data points superimposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(2.9, 7, 500).reshape(-1, 1),\n",
    "        np.linspace(0.8, 2.7, 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "y_proba = ...  # make a prediction here\n",
    "zz = y_proba[:, 1].reshape(x0.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=60, edgecolors='gray')\n",
    "# plot contours and decision boundary\n",
    "left_right = np.array([2.9, 7])\n",
    "boundary = -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1]\n",
    "\n",
    "contour = plt.contour(x0, x1, zz)\n",
    "plt.clabel(contour, inline=1, fontsize=12)\n",
    "plt.plot(left_right, boundary, 'k--', linewidth=3)\n",
    "\n",
    "plt.text(3.5, 1.5, 'Not Iris-Virginica', fontsize=14, ha='center')\n",
    "plt.text(6.5, 2.3, 'Iris-Virginica', fontsize=14, ha='center')\n",
    "plt.xlabel('Petal length', fontsize=14)\n",
    "plt.ylabel('Petal width', fontsize=14)\n",
    "plt.axis([2.9, 7, 0.8, 2.7])\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.savefig('logistic_regression_iris.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax regression\n",
    "\n",
    "To extend the logistic regression to multiple class (multinomial logistic regression), we now study Softmax regression. Here $k$ represent a class index with $K$ classes, $0\\leq k < K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris['data'][:, (2, 3)]  # petal length, petal width\n",
    "y = iris['target']\n",
    "\n",
    "softmax_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10, random_state=42)\n",
    "softmax_reg.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(0, 8, 500).reshape(-1, 1),\n",
    "        np.linspace(0, 3.5, 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "\n",
    "y_proba = softmax_reg.predict_proba(X_new)\n",
    "y_predict = softmax_reg.predict(X_new)\n",
    "zz1 = y_proba[:, 1].reshape(x0.shape)\n",
    "zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "custom_cmap = ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])\n",
    "labels = ['Iris-Virginica', 'Iris-Versicolor', 'Iris-Setosa']\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.contourf(x0, x1, zz, alpha=0.5, cmap=custom_cmap)\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=60, edgecolors='gray', label=labels)\n",
    "contour = plt.contour(x0, x1, zz1, cmap='gray')\n",
    "plt.clabel(contour, inline=1, fontsize=12)\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=labels, title='Class')\n",
    "\n",
    "plt.xlabel('Petal length', fontsize=14)\n",
    "plt.ylabel('Petal width', fontsize=14)\n",
    "#plt.legend(loc='center left', fontsize=14)\n",
    "plt.axis([0, 7, 0, 3.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating fatigue life using SVM regression\n",
    "\n",
    "In this section we will study a fatigue dataset where for each sample we know the defect size, the distance from the surface (from fracture surface observation), the macroscopic stress level and the number of cycles to failure. We will then derive a fatigue model using Support Vector Machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt, colors as mcolors\n",
    "tab = mcolors.TABLEAU_COLORS\n",
    "blue = tab['tab:blue']\n",
    "red = tab['tab:red']\n",
    "orange = tab['tab:orange']\n",
    "green = tab['tab:green']\n",
    "gray = tab['tab:gray']\n",
    "\n",
    "# load the data set using pandas\n",
    "data_path = 'fatigue_defects.csv'\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sq_CS'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['d_surf'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now observe the distribution of the N_cycles feature and find out that we better use log(N_cycles) for our machine learning algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "have a look at the data by plotting the nominal stress as a function of the defect size and using the number of cycles to failure as a color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(..., ..., c=-np.log10(data['N_cycles']), cmap='jet')\n",
    "plt.xlabel('Defect size (mm)')\n",
    "plt.ylabel('Stress (MPa)')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the first three columns as the features and the last one as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ...\n",
    "y = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=101)\n",
    "\n",
    "# normalize de data using a simple MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(...)\n",
    "X_test = scaler.fit_transform(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a first model\n",
    "model = SVR(kernel='poly', C=10., gamma=0.2, epsilon=0.01)\n",
    "model.fit(..., ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions with our trained model\n",
    "y_train_pred = ...\n",
    "y_test_pred = ...\n",
    "y_train = np.exp(y_train)\n",
    "y_test = np.exp(y_test)\n",
    "linear = np.linspace(1e3, 1e8, 10)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(y_train, y_train_pred, 'o', color=gray, label='Train set')\n",
    "plt.plot(y_test, y_test_pred, 'o', color=orange, label='test set')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.plot(linear, linear, '-', color='k', label='ideal')\n",
    "plt.plot(3*linear, linear, '--', color='k', label='3x boundary')\n",
    "plt.plot(linear, 3*linear, '--', color='k')\n",
    "plt.xlabel('N_real')\n",
    "plt.ylabel('N_predicted')\n",
    "plt.title('SVM learning with C={}, gamma={}, epsilon={}'.format(model.C, model.gamma, model.epsilon))\n",
    "plt.legend()\n",
    "plt.axis([1e3, 1e8, 1e3, 1e8])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform a grid search with 5-fold cross validation to tune the hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=101)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "parameters = [{'kernel': ['poly', 'rbf'], \n",
    "               'gamma': [0.01, 0.1, 0.15, 0.2, .05],\n",
    "               'C': [10.0, 12.5, 15, 17.5, 100, 1000],\n",
    "               'epsilon': [0.0001, 0.001, 0.01, 0.05, 0.1, 0.5, 1.0]}]\n",
    "\n",
    "grid = GridSearchCV(model, parameters, cv=5, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best hyper parameters:\", grid.best_estimator_)\n",
    "print(grid.estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions with our optimized model\n",
    "y_train_pred = ...\n",
    "y_test_pred = ...\n",
    "y_train = np.exp(y_train)\n",
    "y_test = np.exp(y_test)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(y_train, y_train_pred, 'o', color=gray, label='Train set')\n",
    "plt.plot(y_test, y_test_pred, 'o', color=orange, label='test set')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.plot(linear, linear, '-', color='k', label='ideal')\n",
    "plt.plot(3*linear, linear, '--', color='k', label='3x boundary')\n",
    "plt.plot(linear, 3*linear, '--', color='k')\n",
    "plt.xlabel('N_real')\n",
    "plt.ylabel('N_predicted')\n",
    "plt.title('SVM learning with C={}, gamma={}, epsilon={}'.format(model.C, model.gamma, model.epsilon))\n",
    "plt.legend()\n",
    "plt.axis([1e3, 1e8, 1e3, 1e8])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the identified model, answer the following questions : \n",
    " 1. What would be the life of a fatigue specimen with a defect of cross section 0.62 mm² located at 0.5 mm from the surface and stressed at 400 MPa?\n",
    " 2. What if the defect size is now 0.31 mm ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-run the code to have access to the data scaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=101)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame with our two data points to use the standard scaler\n",
    "x = pd.DataFrame(...), columns=X.columns)\n",
    "x = scaler.transform(...)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the life for each case\n",
    "np.exp(...).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, build the S-N curve for these two defects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = np.ones((31, 3))\n",
    "for i in range(input1.shape[0]):\n",
    "    input1[i] = [0.62, 0.5, 200. + 10 * i]\n",
    "input2 = np.ones((31, 3))\n",
    "for i in range(input2.shape[0]):\n",
    "    input2[i] = [0.32, 0.5, 200. + 10 * i]\n",
    "\n",
    "x1 = pd.DataFrame(..., columns=X.columns)\n",
    "x2 = pd.DataFrame(..., columns=X.columns)\n",
    "n_cycles_pred1 = np.exp(...)\n",
    "n_cycles_pred2 = np.exp(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_cycles_pred1, x1['stress'], label='defect 0.62 mm')\n",
    "plt.plot(n_cycles_pred2, x2['stress'], label='defect 0.31 mm')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Fatigue life predicted')\n",
    "plt.ylabel('Stress (MPa)')\n",
    "plt.legend()\n",
    "plt.title('S-N curve for two different defects')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
